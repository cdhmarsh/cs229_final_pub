{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This file contains the code to run the baseline experiments.\n",
    "\n",
    "More specifically, we are investigating the performance of basic models on the CIFAR-10 and CIFAR-10H datasets. The tasks for these datasets are multi-class classification.\n",
    "\n",
    "The basic models include:\n",
    "    * ResNet-50\n",
    "    * VGG-16\n",
    "    * Logistic Regression\n",
    "    * Random Forest\n",
    "    * XGBoost\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-10H dataset and return a Dataset\n",
    "def load_cifar10h() -> Dataset:\n",
    "    cifar10h_probs_path = \"../data/cifar-10h/cifar10h-probs.npy\"\n",
    "    if not os.path.exists(cifar10h_probs_path):\n",
    "        raise FileNotFoundError(f\"Soft labels not found at {cifar10h_probs_path}. Please ensure the CIFAR-10H data is downloaded.\")\n",
    "\n",
    "    cifar10h_probs = np.load(cifar10h_probs_path).astype(np.float32)\n",
    "    cifar10_test = datasets.CIFAR10(\n",
    "        root=\"../data/cifar-10\", train=False, download=True, transform=transforms.ToTensor()\n",
    "    )\n",
    "\n",
    "    class CIFAR10H(Dataset):\n",
    "        def __init__(self, cifar10_dataset: Dataset, soft_labels: np.ndarray):\n",
    "            self.cifar10_dataset = cifar10_dataset\n",
    "            self.soft_labels = soft_labels\n",
    "\n",
    "        def __len__(self) -> int:\n",
    "            return len(self.cifar10_dataset)\n",
    "\n",
    "        def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "            image, _ = self.cifar10_dataset[idx]\n",
    "            soft_label = torch.from_numpy(self.soft_labels[idx])\n",
    "            return image.float(), soft_label\n",
    "\n",
    "    cifar10h_dataset = CIFAR10H(cifar10_test, cifar10h_probs)\n",
    "    return cifar10h_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-10 dataset and return train, validation, and test DataLoaders\n",
    "def load_cifar10() -> Tuple[Dataset, Dataset, Dataset]:\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.ConvertImageDtype(torch.float32),\n",
    "        ]\n",
    "    )\n",
    "    full_dataset = datasets.CIFAR10(root=\"../data/cifar-10\", train=True, download=True, transform=transform)\n",
    "\n",
    "    # This dataset will be used for testing and validation.\n",
    "    #   30% of the data will be used for validation, and 70% for testing.\n",
    "    test_size = int(0.7 * len(full_dataset))\n",
    "    val_size = len(full_dataset) - test_size\n",
    "    test_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        full_dataset, [test_size, val_size], generator=torch.Generator().manual_seed(229)\n",
    "    )\n",
    "\n",
    "    return test_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ml_data(cifar10h_dataset, cifar10_test_dataset):\n",
    "    # Prepare data for ML models\n",
    "    X_cifar10h = np.array([img.numpy().flatten() for img, _ in cifar10h_dataset])\n",
    "    y_cifar10h = np.array([np.argmax(label) for _, label in cifar10h_dataset])\n",
    "\n",
    "    X_cifar10_test = np.array([img.numpy().flatten() for img, _ in cifar10_test_dataset])\n",
    "    y_cifar10_test = np.array([label for _, label in cifar10_test_dataset])\n",
    "\n",
    "    # Scale the data for ML models\n",
    "    scaler = StandardScaler()\n",
    "    X_cifar10h_scaled = scaler.fit_transform(X_cifar10h)\n",
    "    X_cifar10_scaled_test = scaler.transform(X_cifar10_test)\n",
    "\n",
    "    return X_cifar10h_scaled, y_cifar10h, X_cifar10_scaled_test, y_cifar10_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "CIFAR-10H dataset loaded with 10000 samples\n",
      "Files already downloaded and verified\n",
      "CIFAR-10 dataset loaded with 35000 test and 15000 validation samples\n"
     ]
    }
   ],
   "source": [
    "cifar10h_dataset = load_cifar10h()\n",
    "cifar10h_loader = DataLoader(cifar10h_dataset, batch_size=128, shuffle=True)\n",
    "print(f\"CIFAR-10H dataset loaded with {len(cifar10h_dataset)} samples\")\n",
    "\n",
    "cifar10_test_dataset, cifar10_val_dataset = load_cifar10()  # Changed variable name to reflect split\n",
    "cifar10_test_loader = DataLoader(cifar10_test_dataset, batch_size=128, shuffle=False)\n",
    "cifar10_val_loader = DataLoader(cifar10_val_dataset, batch_size=128, shuffle=False)\n",
    "print(\n",
    "    f\"CIFAR-10 dataset loaded with {len(cifar10_test_dataset)} test and {len(cifar10_val_dataset)} validation samples\"\n",
    ")\n",
    "X_cifar10h, y_cifar10h, X_cifar10_test, y_cifar10_test = get_ml_data(\n",
    "    cifar10h_dataset, cifar10_test_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "Training is done on the CIFAR-10H dataset. Evaluation is done on the CIFAR-10 train set, which we use as a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    optimizer: optim.Optimizer,\n",
    "    num_epochs: int,\n",
    ") -> nn.Module:\n",
    "    device = torch.device(\n",
    "        \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    )\n",
    "    print(f\"Using device: {device}\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            if (i + 1) % 50 == 0:\n",
    "                print(\n",
    "                    f\"  Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Train Loss: {running_loss/50:.4f}\"\n",
    "                )\n",
    "                running_loss = 0.0\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(images)\n",
    "\n",
    "                if len(labels.shape) > 1:  # For soft labels\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    _, labels = torch.max(labels, 1)\n",
    "                else:  # For hard labels\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                val_loss += criterion(outputs, labels).item()\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] Validation Loss: {val_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn_model(\n",
    "    model, cifar10h_loader: DataLoader, cifar10_val_loader: DataLoader, num_epochs: int = 20, lr: float = 0.001\n",
    ") -> list:\n",
    "    print(f\"\\nTraining {model.__class__.__name__} on CIFAR-10H...\")\n",
    "\n",
    "    # Adjust the final layer for CIFAR-10\n",
    "    if isinstance(model, models.ResNet):\n",
    "        num_ftrs = model.fc.in_features\n",
    "        model.fc = nn.Linear(num_ftrs, 10)\n",
    "    elif isinstance(model, models.VGG):\n",
    "        num_ftrs = model.classifier[-1].in_features\n",
    "        model.classifier[-1] = nn.Linear(num_ftrs, 10)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    model = train_model(\n",
    "        model=model,\n",
    "        train_loader=cifar10h_loader,\n",
    "        val_loader=cifar10_val_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        num_epochs=num_epochs,\n",
    "    )\n",
    "    torch.save(model.state_dict(), f\"models/{model.__class__.__name__}_cifar10h.pth\")\n",
    "\n",
    "def evaluate_nn_model(model, cifar10_test_loader):\n",
    "    model.load_state_dict(\n",
    "        torch.load(f\"models/{model.__class__.__name__}_cifar10h.pth\", weights_only=True)\n",
    "    )\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    device = next(model.parameters()).device\n",
    "    with torch.no_grad():\n",
    "        for images, labels in cifar10_test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"{model.__class__.__name__} Accuracy on CIFAR-10 test set: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /Users/jackle/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n",
      "100%|██████████| 83.3M/83.3M [00:02<00:00, 29.6MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training ResNet on CIFAR-10H...\n",
      "Using device: mps\n",
      "  Epoch [1/20], Step [50/79], Train Loss: 1.4200\n",
      "Epoch [1/20] Validation Loss: 1.3637, Accuracy: 55.83%\n",
      "  Epoch [2/20], Step [50/79], Train Loss: 0.8788\n",
      "Epoch [2/20] Validation Loss: 0.9484, Accuracy: 68.19%\n",
      "  Epoch [3/20], Step [50/79], Train Loss: 0.6900\n",
      "Epoch [3/20] Validation Loss: 0.9948, Accuracy: 67.32%\n",
      "  Epoch [4/20], Step [50/79], Train Loss: 0.5706\n",
      "Epoch [4/20] Validation Loss: 0.8675, Accuracy: 71.50%\n",
      "  Epoch [5/20], Step [50/79], Train Loss: 0.4484\n",
      "Epoch [5/20] Validation Loss: 0.9914, Accuracy: 69.73%\n",
      "  Epoch [6/20], Step [50/79], Train Loss: 0.4166\n",
      "Epoch [6/20] Validation Loss: 0.8313, Accuracy: 73.81%\n",
      "  Epoch [7/20], Step [50/79], Train Loss: 0.4293\n",
      "Epoch [7/20] Validation Loss: 0.9724, Accuracy: 69.63%\n",
      "  Epoch [8/20], Step [50/79], Train Loss: 0.3694\n",
      "Epoch [8/20] Validation Loss: 0.8233, Accuracy: 74.32%\n",
      "  Epoch [9/20], Step [50/79], Train Loss: 0.3564\n",
      "Epoch [9/20] Validation Loss: 0.8398, Accuracy: 73.51%\n",
      "  Epoch [10/20], Step [50/79], Train Loss: 0.2687\n",
      "Epoch [10/20] Validation Loss: 0.7453, Accuracy: 76.45%\n",
      "  Epoch [11/20], Step [50/79], Train Loss: 0.2462\n",
      "Epoch [11/20] Validation Loss: 0.7412, Accuracy: 76.56%\n",
      "  Epoch [12/20], Step [50/79], Train Loss: 0.2610\n",
      "Epoch [12/20] Validation Loss: 0.8486, Accuracy: 73.99%\n",
      "  Epoch [13/20], Step [50/79], Train Loss: 0.2742\n",
      "Epoch [13/20] Validation Loss: 0.8297, Accuracy: 74.68%\n",
      "  Epoch [14/20], Step [50/79], Train Loss: 0.2517\n",
      "Epoch [14/20] Validation Loss: 0.7465, Accuracy: 77.12%\n",
      "  Epoch [15/20], Step [50/79], Train Loss: 0.2593\n",
      "Epoch [15/20] Validation Loss: 0.8188, Accuracy: 74.42%\n",
      "  Epoch [16/20], Step [50/79], Train Loss: 0.2499\n",
      "Epoch [16/20] Validation Loss: 0.7872, Accuracy: 75.65%\n",
      "  Epoch [17/20], Step [50/79], Train Loss: 0.2356\n",
      "Epoch [17/20] Validation Loss: 0.8314, Accuracy: 74.34%\n",
      "  Epoch [18/20], Step [50/79], Train Loss: 0.2271\n",
      "Epoch [18/20] Validation Loss: 0.7643, Accuracy: 76.56%\n",
      "  Epoch [19/20], Step [50/79], Train Loss: 0.4032\n",
      "Epoch [19/20] Validation Loss: 0.9137, Accuracy: 71.43%\n",
      "  Epoch [20/20], Step [50/79], Train Loss: 0.3512\n",
      "Epoch [20/20] Validation Loss: 1.0684, Accuracy: 69.54%\n",
      "ResNet Accuracy on CIFAR-10 test set: 69.45%\n"
     ]
    }
   ],
   "source": [
    "resnet_model = models.resnet34(weights=models.ResNet34_Weights.DEFAULT)\n",
    "train_nn_model(resnet_model, cifar10h_loader, cifar10_val_loader, lr=0.01)\n",
    "evaluate_nn_model(resnet_model, cifar10_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training VGG on CIFAR-10H...\n",
      "Using device: mps\n",
      "  Epoch [1/20], Step [50/79], Train Loss: 2.4263\n",
      "Epoch [1/20] Validation Loss: 2.2472, Accuracy: 12.31%\n",
      "  Epoch [2/20], Step [50/79], Train Loss: 2.2072\n",
      "Epoch [2/20] Validation Loss: 2.0230, Accuracy: 17.27%\n",
      "  Epoch [3/20], Step [50/79], Train Loss: 1.9179\n",
      "Epoch [3/20] Validation Loss: 1.7945, Accuracy: 27.06%\n",
      "  Epoch [4/20], Step [50/79], Train Loss: 1.8446\n",
      "Epoch [4/20] Validation Loss: 1.8051, Accuracy: 26.39%\n",
      "  Epoch [5/20], Step [50/79], Train Loss: 1.7638\n",
      "Epoch [5/20] Validation Loss: 1.6950, Accuracy: 32.66%\n",
      "  Epoch [6/20], Step [50/79], Train Loss: 1.6645\n",
      "Epoch [6/20] Validation Loss: 1.5061, Accuracy: 38.39%\n",
      "  Epoch [7/20], Step [50/79], Train Loss: 1.5940\n",
      "Epoch [7/20] Validation Loss: 1.4741, Accuracy: 40.19%\n",
      "  Epoch [8/20], Step [50/79], Train Loss: 1.4450\n",
      "Epoch [8/20] Validation Loss: 1.5057, Accuracy: 44.57%\n",
      "  Epoch [9/20], Step [50/79], Train Loss: 1.4280\n",
      "Epoch [9/20] Validation Loss: 1.3142, Accuracy: 48.93%\n",
      "  Epoch [10/20], Step [50/79], Train Loss: 1.3082\n",
      "Epoch [10/20] Validation Loss: 1.2835, Accuracy: 51.61%\n",
      "  Epoch [11/20], Step [50/79], Train Loss: 1.2275\n",
      "Epoch [11/20] Validation Loss: 1.6161, Accuracy: 40.34%\n",
      "  Epoch [12/20], Step [50/79], Train Loss: 1.2888\n",
      "Epoch [12/20] Validation Loss: 1.2877, Accuracy: 53.87%\n",
      "  Epoch [13/20], Step [50/79], Train Loss: 1.0705\n",
      "Epoch [13/20] Validation Loss: 1.2695, Accuracy: 54.90%\n",
      "  Epoch [14/20], Step [50/79], Train Loss: 1.0443\n",
      "Epoch [14/20] Validation Loss: 1.1909, Accuracy: 58.73%\n",
      "  Epoch [15/20], Step [50/79], Train Loss: 0.8542\n",
      "Epoch [15/20] Validation Loss: 0.9965, Accuracy: 66.89%\n",
      "  Epoch [16/20], Step [50/79], Train Loss: 0.7688\n",
      "Epoch [16/20] Validation Loss: 0.9521, Accuracy: 67.93%\n",
      "  Epoch [17/20], Step [50/79], Train Loss: 0.9680\n",
      "Epoch [17/20] Validation Loss: 1.2176, Accuracy: 61.15%\n",
      "  Epoch [18/20], Step [50/79], Train Loss: 0.7614\n",
      "Epoch [18/20] Validation Loss: 0.9317, Accuracy: 69.24%\n"
     ]
    }
   ],
   "source": [
    "vgg_model = models.vgg16(weights=models.VGG16_Weights.DEFAULT)\n",
    "train_nn_model(vgg_model, cifar10h_loader, cifar10_val_loader, lr=0.001)\n",
    "evaluate_nn_model(vgg_model, cifar10_test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ml_models(model, X_cifar10h_scaled, y_cifar10h):\n",
    "    # Machine Learning models\n",
    "\n",
    "    print(f\"\\nTraining {model.__class__.__name__} on CIFAR-10H...\")\n",
    "    model.fit(X_cifar10h_scaled, y_cifar10h)  # Use scaled data\n",
    "\n",
    "def evaluate_ml_models(model, X_cifar10_scaled, y_cifar10):\n",
    "    y_pred = model.predict(X_cifar10_scaled)  # Use scaled data\n",
    "    accuracy = accuracy_score(y_cifar10, y_pred)\n",
    "    accuracy = 100 * accuracy\n",
    "    print(f\"{model.__class__.__name__} Accuracy on CIFAR-10 test set: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_model = LogisticRegression(max_iter=3000, n_jobs=-1)\n",
    "train_ml_models(logistic_model, X_cifar10h, y_cifar10h)\n",
    "evaluate_ml_models(logistic_model, X_cifar10_test, y_cifar10_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_model = RandomForestClassifier(n_jobs=-1)\n",
    "train_ml_models(random_forest_model, X_cifar10h, y_cifar10h)\n",
    "evaluate_ml_models(random_forest_model, X_cifar10_test, y_cifar10_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = XGBClassifier(n_jobs=-1)\n",
    "train_ml_models(xgb_model, X_cifar10h, y_cifar10h)\n",
    "evaluate_ml_models(xgb_model, X_cifar10_test, y_cifar10_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs229_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
