{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This file contains the code to run the baseline experiments.\n",
    "\n",
    "More specifically, we are investigating the performance of basic models on the CIFAR-10 and CIFAR-10H datasets. The tasks for these datasets are multi-class classification.\n",
    "\n",
    "The basic models include:\n",
    "    * ResNet-50\n",
    "    * VGG-16\n",
    "    * Logistic Regression\n",
    "    * Random Forest\n",
    "    * XGBoost\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-10 dataset and return train, validation, and test DataLoaders\n",
    "def load_cifar10() -> Tuple[Dataset, Dataset, Dataset]:\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.ConvertImageDtype(torch.float32),\n",
    "        ]\n",
    "    )\n",
    "    full_dataset = datasets.CIFAR10(root=\"../data/cifar-10\", train=True, download=True, transform=transform)\n",
    "    # we use the test dataset for training, similar to the CIFAR-10H experiment\n",
    "    train_dataset = datasets.CIFAR10(root=\"../data/cifar-10\", train=False, download=True, transform=transform)\n",
    "\n",
    "    # This dataset will be used for testing and validation.\n",
    "    #   30% of the data will be used for validation, and 70% for testing.\n",
    "    test_size = int(0.7 * len(full_dataset))\n",
    "    val_size = len(full_dataset) - test_size\n",
    "    test_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        full_dataset, [test_size, val_size], generator=torch.Generator().manual_seed(229)\n",
    "    )\n",
    "\n",
    "    return train_dataset, test_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ml_data(cifar10_train_dataset, cifar10_test_dataset):\n",
    "    # Prepare data for ML models\n",
    "    X_cifar10 = np.array([img.numpy().flatten() for img, _ in cifar10_train_dataset])\n",
    "    y_cifar10 = np.array([np.argmax(label) for _, label in cifar10_train_dataset])\n",
    "\n",
    "    X_cifar10_test = np.array([img.numpy().flatten() for img, _ in cifar10_test_dataset])\n",
    "    y_cifar10_test = np.array([label for _, label in cifar10_test_dataset])\n",
    "\n",
    "    # Scale the data for ML models\n",
    "    scaler = StandardScaler()\n",
    "    X_cifar10_scaled = scaler.fit_transform(X_cifar10)\n",
    "    X_cifar10_scaled_test = scaler.transform(X_cifar10_test)\n",
    "\n",
    "    return X_cifar10_scaled, y_cifar10, X_cifar10_scaled_test, y_cifar10_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "CIFAR-10 dataset loaded with 10000 training, 35000 test, and 15000 validation samples\n"
     ]
    }
   ],
   "source": [
    "cifar10_train_dataset, cifar10_test_dataset, cifar10_val_dataset = load_cifar10() \n",
    "cifar10_train_loader = DataLoader(cifar10_train_dataset, batch_size=128, shuffle=True)\n",
    "cifar10_test_loader = DataLoader(cifar10_test_dataset, batch_size=128, shuffle=False)\n",
    "cifar10_val_loader = DataLoader(cifar10_val_dataset, batch_size=128, shuffle=False)\n",
    "print(\n",
    "    f\"CIFAR-10 dataset loaded with {len(cifar10_train_dataset)} training, {len(cifar10_test_dataset)} test, and {len(cifar10_val_dataset)} validation samples\"\n",
    ")\n",
    "X_cifar10, y_cifar10, X_cifar10_test, y_cifar10_test = get_ml_data(cifar10_train_dataset, cifar10_test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "Training is done on the CIFAR-10 test set. Evaluation is done on the CIFAR-10 train set, which we use as a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    optimizer: optim.Optimizer,\n",
    "    num_epochs: int,\n",
    ") -> nn.Module:\n",
    "    device = torch.device(\n",
    "        \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    )\n",
    "    print(f\"Using device: {device}\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(images)\n",
    "\n",
    "                if len(labels.shape) > 1:  # For soft labels\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    _, labels = torch.max(labels, 1)\n",
    "                else:  # For hard labels\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                val_loss += criterion(outputs, labels).item()\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        print(\n",
    "            f\"Epoch [{epoch+1}/{num_epochs}] Train Loss: {running_loss/len(train_loader):.4f}, Validation Loss: {val_loss:.4f}, Accuracy: {accuracy:.2f}%\"\n",
    "        )\n",
    "\n",
    "        # Save model if validation accuracy improves\n",
    "        if accuracy > best_val_acc:\n",
    "            best_val_acc = accuracy\n",
    "            torch.save(model.state_dict(), f\"models/{model.__class__.__name__}_cifar10.pth\")\n",
    "            print(f\"Saved model with improved validation accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn_model(\n",
    "    model, cifar10_train_loader: DataLoader, cifar10_val_loader: DataLoader, num_epochs: int = 20, lr: float = 0.001\n",
    ") -> list:\n",
    "    print(f\"\\nTraining {model.__class__.__name__} on CIFAR-10...\")\n",
    "\n",
    "    # Adjust the final layer for CIFAR-10\n",
    "    if isinstance(model, models.ResNet):\n",
    "        num_ftrs = model.fc.in_features\n",
    "        model.fc = nn.Linear(num_ftrs, 10)\n",
    "    elif isinstance(model, models.VGG):\n",
    "        num_ftrs = model.classifier[-1].in_features\n",
    "        model.classifier[-1] = nn.Linear(num_ftrs, 10)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    model = train_model(\n",
    "        model=model,\n",
    "        train_loader=cifar10_train_loader,\n",
    "        val_loader=cifar10_val_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        num_epochs=num_epochs,\n",
    "    )\n",
    "\n",
    "def evaluate_nn_model(model, cifar10_test_loader):\n",
    "    model.load_state_dict(\n",
    "        torch.load(f\"models/{model.__class__.__name__}_cifar10.pth\", weights_only=True)\n",
    "    )\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    device = next(model.parameters()).device\n",
    "    with torch.no_grad():\n",
    "        for images, labels in cifar10_test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"{model.__class__.__name__} Accuracy on CIFAR-10 test set: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training ResNet on CIFAR-10...\n",
      "Using device: mps\n",
      "Epoch [1/20] Train Loss: 1.3250, Validation Loss: 1.1499, Accuracy: 61.73%\n",
      "Saved model with improved validation accuracy: 61.73%\n",
      "Epoch [2/20] Train Loss: 0.8357, Validation Loss: 1.2902, Accuracy: 58.62%\n",
      "Epoch [3/20] Train Loss: 0.6018, Validation Loss: 0.9993, Accuracy: 67.25%\n",
      "Saved model with improved validation accuracy: 67.25%\n",
      "Epoch [4/20] Train Loss: 0.4476, Validation Loss: 0.9803, Accuracy: 69.31%\n",
      "Saved model with improved validation accuracy: 69.31%\n",
      "Epoch [5/20] Train Loss: 0.3473, Validation Loss: 1.2525, Accuracy: 64.48%\n",
      "Epoch [6/20] Train Loss: 0.3187, Validation Loss: 1.0744, Accuracy: 69.83%\n",
      "Saved model with improved validation accuracy: 69.83%\n",
      "Epoch [7/20] Train Loss: 0.2155, Validation Loss: 1.0951, Accuracy: 70.47%\n",
      "Saved model with improved validation accuracy: 70.47%\n",
      "Epoch [8/20] Train Loss: 0.2564, Validation Loss: 1.0781, Accuracy: 71.43%\n",
      "Saved model with improved validation accuracy: 71.43%\n",
      "Epoch [9/20] Train Loss: 0.1118, Validation Loss: 1.1679, Accuracy: 72.10%\n",
      "Saved model with improved validation accuracy: 72.10%\n",
      "Epoch [10/20] Train Loss: 0.0835, Validation Loss: 1.4002, Accuracy: 69.92%\n",
      "Epoch [11/20] Train Loss: 0.1758, Validation Loss: 1.1557, Accuracy: 71.63%\n",
      "Epoch [12/20] Train Loss: 0.1074, Validation Loss: 1.2700, Accuracy: 71.14%\n",
      "Epoch [13/20] Train Loss: 0.1222, Validation Loss: 1.1372, Accuracy: 72.38%\n",
      "Saved model with improved validation accuracy: 72.38%\n",
      "Epoch [14/20] Train Loss: 0.1178, Validation Loss: 1.2617, Accuracy: 70.36%\n",
      "Epoch [15/20] Train Loss: 0.1438, Validation Loss: 1.2655, Accuracy: 71.67%\n",
      "Epoch [16/20] Train Loss: 0.2953, Validation Loss: 1.1592, Accuracy: 70.97%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m resnet_model \u001b[38;5;241m=\u001b[39m models\u001b[38;5;241m.\u001b[39mresnet34(weights\u001b[38;5;241m=\u001b[39mmodels\u001b[38;5;241m.\u001b[39mResNet34_Weights\u001b[38;5;241m.\u001b[39mDEFAULT)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain_nn_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresnet_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcifar10_train_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcifar10_val_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m evaluate_nn_model(resnet_model, cifar10_test_loader)\n",
      "Cell \u001b[0;32mIn[6], line 17\u001b[0m, in \u001b[0;36mtrain_nn_model\u001b[0;34m(model, cifar10_train_loader, cifar10_val_loader, num_epochs, lr)\u001b[0m\n\u001b[1;32m     14\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m     15\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcifar10_train_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcifar10_val_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 29\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m     26\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     27\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 29\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Validation phase\u001b[39;00m\n\u001b[1;32m     32\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "resnet_model = models.resnet34(weights=models.ResNet34_Weights.DEFAULT)\n",
    "train_nn_model(resnet_model, cifar10_train_loader, cifar10_val_loader, lr=0.01)\n",
    "evaluate_nn_model(resnet_model, cifar10_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_model = models.vgg16(weights=models.VGG16_Weights.DEFAULT)\n",
    "train_nn_model(vgg_model, cifar10_train_loader, cifar10_val_loader, lr=0.001)\n",
    "evaluate_nn_model(vgg_model, cifar10_test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ml_models(model, X_cifar10_scaled, y_cifar10):\n",
    "    # Machine Learning models\n",
    "\n",
    "    print(f\"\\nTraining {model.__class__.__name__} on CIFAR-10...\")\n",
    "    model.fit(X_cifar10_scaled, y_cifar10)  # Use scaled data\n",
    "\n",
    "\n",
    "def evaluate_ml_models(model, X_cifar10_scaled, y_cifar10):\n",
    "    y_pred = model.predict(X_cifar10_scaled)  # Use scaled data\n",
    "    accuracy = accuracy_score(y_cifar10, y_pred)\n",
    "    accuracy = 100 * accuracy\n",
    "    print(f\"{model.__class__.__name__} Accuracy on CIFAR-10 test set: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_model = LogisticRegression(max_iter=3000, n_jobs=-1)\n",
    "train_ml_models(logistic_model, X_cifar10, y_cifar10)\n",
    "evaluate_ml_models(logistic_model, X_cifar10_test, y_cifar10_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_model = RandomForestClassifier(n_jobs=-1)\n",
    "train_ml_models(random_forest_model, X_cifar10, y_cifar10)\n",
    "evaluate_ml_models(random_forest_model, X_cifar10_test, y_cifar10_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = XGBClassifier(n_jobs=-1)\n",
    "train_ml_models(xgb_model, X_cifar10, y_cifar10)\n",
    "evaluate_ml_models(xgb_model, X_cifar10_test, y_cifar10_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs229_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
