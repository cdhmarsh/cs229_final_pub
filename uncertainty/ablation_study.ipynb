{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This file contains the code to run experiments with artificial soft labels.\n",
    "\n",
    "The experiment is:\n",
    "    * Train a soft label predictor model on CIFAR-10H\n",
    "    * Generate artificial soft labels for CIFAR-10\n",
    "    * Train a model on CIFAR-10 with the artificial soft labels + CIFAR-10H\n",
    "    * Evaluate the model on CIFAR-10\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from torchvision import datasets, transforms, models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from typing import Tuple\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-10 dataset and return augment, train, validation, and test DataLoaders\n",
    "def load_cifar10_experiment():\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.ConvertImageDtype(torch.float32),\n",
    "        ]\n",
    "    )\n",
    "    full_dataset = datasets.CIFAR10(root=\"../data/cifar-10\", train=True, download=True, transform=transform)\n",
    "    # we use the test dataset for training, similar to the CIFAR-10H experiment\n",
    "    train_dataset = datasets.CIFAR10(root=\"../data/cifar-10\", train=False, download=True, transform=transform)\n",
    "\n",
    "    # This dataset will be used for augmenting, testing, and validation.\n",
    "    augment_size = int(0.7 * len(full_dataset))\n",
    "    val_size = (len(full_dataset) - augment_size) // 2\n",
    "    test_size = len(full_dataset) - augment_size - val_size\n",
    "    augment_dataset, test_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        full_dataset, [augment_size, test_size, val_size], generator=torch.Generator().manual_seed(229)\n",
    "    )\n",
    "\n",
    "    return augment_dataset, train_dataset, test_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "CIFAR-10 dataset loaded with 35000 augment, 10000 training, 7500 test, and 7500 validation samples\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    cifar10_hard_augment_dataset,\n",
    "    cifar10_hard_train_dataset,\n",
    "    cifar10_hard_test_dataset,\n",
    "    cifar10_hard_val_dataset,\n",
    ") = load_cifar10_experiment()\n",
    "\n",
    "cifar10_hard_test_loader = DataLoader(cifar10_hard_test_dataset, batch_size=128, shuffle=False)\n",
    "cifar10_hard_val_loader = DataLoader(cifar10_hard_val_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "print(\n",
    "    f\"CIFAR-10 dataset loaded with {len(cifar10_hard_augment_dataset)} augment, {len(cifar10_hard_train_dataset)} training, {len(cifar10_hard_test_dataset)} test, and {len(cifar10_hard_val_dataset)} validation samples\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "Training is done on the CIFAR-10H dataset. Evaluation is done on the CIFAR-10 train set, which we use as a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    optimizer: optim.Optimizer,\n",
    "    num_epochs: int,\n",
    "    model_path,\n",
    ") -> nn.Module:\n",
    "    device = torch.device(\n",
    "        \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    )\n",
    "    print(f\"Using device: {device}\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(images)\n",
    "\n",
    "                if len(labels.shape) > 1:  # For soft labels\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    _, labels = torch.max(labels, 1)\n",
    "                else:  # For hard labels\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                val_loss += criterion(outputs, labels).item()\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        print(\n",
    "            f\"Epoch [{epoch+1}/{num_epochs}] Train Loss: {running_loss/len(train_loader):.4f}, Validation Loss: {val_loss:.4f}, Accuracy: {accuracy:.2f}%\"\n",
    "        )\n",
    "\n",
    "        # Save model if validation accuracy improves\n",
    "        if model_path is not None:\n",
    "            if accuracy > best_val_acc:\n",
    "                best_val_acc = accuracy\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "                print(f\"Saved model with improved validation accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn_model(model, cifar10h_loader, cifar10_val_loader, num_epochs=20, model_path=None):\n",
    "    print(f\"\\nTraining {model.__class__.__name__} on CIFAR-10H...\")\n",
    "\n",
    "    # Adjust the final layer for CIFAR-10\n",
    "    if isinstance(model, models.ResNet):\n",
    "        num_ftrs = model.fc.in_features\n",
    "        model.fc = nn.Linear(num_ftrs, 10)\n",
    "    elif isinstance(model, models.VGG):\n",
    "        num_ftrs = model.classifier[-1].in_features\n",
    "        model.classifier[-1] = nn.Linear(num_ftrs, 10)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    model = train_model(\n",
    "        model=model,\n",
    "        train_loader=cifar10h_loader,\n",
    "        val_loader=cifar10_val_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        num_epochs=num_epochs,\n",
    "        model_path=model_path\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageHardToSoftLabelModel(\n",
       "  (image_encoder): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Sequential(\n",
       "      (0): ResidualBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): ResidualBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): ResidualBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (6): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  )\n",
       "  (label_encoder): Sequential(\n",
       "    (0): Linear(in_features=10, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.3, inplace=False)\n",
       "    (3): Linear(in_features=256, out_features=512, bias=True)\n",
       "  )\n",
       "  (attention): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Dropout(p=0.3, inplace=False)\n",
       "    (4): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): Dropout(p=0.3, inplace=False)\n",
       "    (8): Linear(in_features=256, out_features=10, bias=True)\n",
       "    (9): TemperatureSoftmax()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from generate_soft_labels import create_soft_label_dataloader, create_soft_label_dataset\n",
    "from soft_label_predictor import ImageHardToSoftLabelModel\n",
    "\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "# Load the trained model\n",
    "soft_label_model = ImageHardToSoftLabelModel().to(device)\n",
    "soft_label_model.load_state_dict(torch.load(\"models/soft_label_model.pt\", weights_only=True))\n",
    "soft_label_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new class to handle both hard and soft labels consistently\n",
    "class CIFAR10LabelDataset(Dataset):\n",
    "    def __init__(self, dataset, soft_labels=None):\n",
    "        self.dataset = dataset\n",
    "        self.soft_labels = soft_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.dataset[idx]\n",
    "        if self.soft_labels is None:\n",
    "            # Convert hard labels to one-hot\n",
    "            label = F.one_hot(torch.tensor(label), num_classes=10).float()\n",
    "        else:\n",
    "            label = torch.tensor(self.soft_labels[idx])\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running experiment with 100% soft labels\n",
      "\n",
      "Training ResNet on CIFAR-10H...\n",
      "Using device: mps\n",
      "Epoch [1/30] Train Loss: 0.9667, Validation Loss: 0.9417, Accuracy: 67.97%\n",
      "Saved model with improved validation accuracy: 67.97%\n",
      "Epoch [2/30] Train Loss: 0.6524, Validation Loss: 0.6964, Accuracy: 76.00%\n",
      "Saved model with improved validation accuracy: 76.00%\n",
      "Epoch [3/30] Train Loss: 0.5231, Validation Loss: 0.8131, Accuracy: 74.04%\n",
      "Epoch [4/30] Train Loss: 0.4250, Validation Loss: 0.7036, Accuracy: 77.04%\n",
      "Saved model with improved validation accuracy: 77.04%\n",
      "Epoch [5/30] Train Loss: 0.3324, Validation Loss: 0.6472, Accuracy: 78.99%\n",
      "Saved model with improved validation accuracy: 78.99%\n",
      "Epoch [6/30] Train Loss: 0.2914, Validation Loss: 0.7075, Accuracy: 78.35%\n",
      "Epoch [7/30] Train Loss: 0.2421, Validation Loss: 0.6916, Accuracy: 79.05%\n",
      "Saved model with improved validation accuracy: 79.05%\n",
      "Epoch [8/30] Train Loss: 0.2172, Validation Loss: 0.7136, Accuracy: 79.33%\n",
      "Saved model with improved validation accuracy: 79.33%\n",
      "Epoch [9/30] Train Loss: 0.2043, Validation Loss: 0.7359, Accuracy: 78.36%\n",
      "Epoch [10/30] Train Loss: 0.2078, Validation Loss: 0.6928, Accuracy: 80.12%\n",
      "Saved model with improved validation accuracy: 80.12%\n",
      "Epoch [11/30] Train Loss: 0.1865, Validation Loss: 0.6909, Accuracy: 79.85%\n",
      "Epoch [12/30] Train Loss: 0.1747, Validation Loss: 0.7632, Accuracy: 78.65%\n",
      "Epoch [13/30] Train Loss: 0.2153, Validation Loss: 0.7270, Accuracy: 78.89%\n",
      "Epoch [14/30] Train Loss: 0.2485, Validation Loss: 0.7226, Accuracy: 78.21%\n",
      "Epoch [15/30] Train Loss: 0.1986, Validation Loss: 0.6960, Accuracy: 80.32%\n",
      "Saved model with improved validation accuracy: 80.32%\n",
      "Epoch [16/30] Train Loss: 0.2470, Validation Loss: 0.7889, Accuracy: 77.79%\n",
      "Epoch [17/30] Train Loss: 0.1665, Validation Loss: 0.6771, Accuracy: 81.21%\n",
      "Saved model with improved validation accuracy: 81.21%\n",
      "Epoch [18/30] Train Loss: 0.1390, Validation Loss: 0.6761, Accuracy: 81.69%\n",
      "Saved model with improved validation accuracy: 81.69%\n",
      "Epoch [19/30] Train Loss: 0.1308, Validation Loss: 0.6747, Accuracy: 81.52%\n",
      "Epoch [20/30] Train Loss: 0.1305, Validation Loss: 0.7230, Accuracy: 80.97%\n",
      "Epoch [21/30] Train Loss: 0.1438, Validation Loss: 0.7993, Accuracy: 78.35%\n",
      "Epoch [22/30] Train Loss: 0.1599, Validation Loss: 0.8145, Accuracy: 78.05%\n",
      "Epoch [23/30] Train Loss: 0.1720, Validation Loss: 0.8206, Accuracy: 77.77%\n",
      "Epoch [24/30] Train Loss: 0.1521, Validation Loss: 0.7224, Accuracy: 80.52%\n",
      "Epoch [25/30] Train Loss: 0.1505, Validation Loss: 0.7135, Accuracy: 80.85%\n",
      "Epoch [26/30] Train Loss: 0.1499, Validation Loss: 0.7362, Accuracy: 80.24%\n",
      "Epoch [27/30] Train Loss: 0.1551, Validation Loss: 0.7688, Accuracy: 79.44%\n",
      "Epoch [28/30] Train Loss: 0.1422, Validation Loss: 0.7994, Accuracy: 78.87%\n",
      "Epoch [29/30] Train Loss: 0.1389, Validation Loss: 0.7503, Accuracy: 80.17%\n",
      "Epoch [30/30] Train Loss: 0.1497, Validation Loss: 0.8352, Accuracy: 77.73%\n",
      "\n",
      "Running experiment with 0% soft labels\n",
      "\n",
      "Training ResNet on CIFAR-10H...\n",
      "Using device: mps\n",
      "Epoch [1/30] Train Loss: 0.9549, Validation Loss: 0.8785, Accuracy: 70.71%\n",
      "Saved model with improved validation accuracy: 70.71%\n",
      "Epoch [2/30] Train Loss: 0.6104, Validation Loss: 0.7340, Accuracy: 75.45%\n",
      "Saved model with improved validation accuracy: 75.45%\n",
      "Epoch [3/30] Train Loss: 0.4644, Validation Loss: 0.7096, Accuracy: 76.76%\n",
      "Saved model with improved validation accuracy: 76.76%\n",
      "Epoch [4/30] Train Loss: 0.4059, Validation Loss: 0.7057, Accuracy: 76.83%\n",
      "Saved model with improved validation accuracy: 76.83%\n",
      "Epoch [5/30] Train Loss: 0.2889, Validation Loss: 0.7244, Accuracy: 78.31%\n",
      "Saved model with improved validation accuracy: 78.31%\n",
      "Epoch [6/30] Train Loss: 0.2073, Validation Loss: 0.7442, Accuracy: 78.76%\n",
      "Saved model with improved validation accuracy: 78.76%\n",
      "Epoch [7/30] Train Loss: 0.1570, Validation Loss: 0.7640, Accuracy: 78.68%\n",
      "Epoch [8/30] Train Loss: 0.1366, Validation Loss: 0.8337, Accuracy: 78.41%\n",
      "Epoch [9/30] Train Loss: 0.1024, Validation Loss: 0.9205, Accuracy: 77.88%\n",
      "Epoch [10/30] Train Loss: 0.1015, Validation Loss: 0.9656, Accuracy: 77.53%\n",
      "Epoch [11/30] Train Loss: 0.0828, Validation Loss: 0.8807, Accuracy: 78.39%\n",
      "Epoch [12/30] Train Loss: 0.0765, Validation Loss: 0.9997, Accuracy: 77.93%\n",
      "Epoch [13/30] Train Loss: 0.0736, Validation Loss: 0.9098, Accuracy: 79.43%\n",
      "Saved model with improved validation accuracy: 79.43%\n",
      "Epoch [14/30] Train Loss: 0.0595, Validation Loss: 1.0812, Accuracy: 77.69%\n",
      "Epoch [15/30] Train Loss: 0.1558, Validation Loss: 0.8513, Accuracy: 78.15%\n",
      "Epoch [16/30] Train Loss: 0.0793, Validation Loss: 0.9137, Accuracy: 79.31%\n",
      "Epoch [17/30] Train Loss: 0.0466, Validation Loss: 0.9615, Accuracy: 79.85%\n",
      "Saved model with improved validation accuracy: 79.85%\n",
      "Epoch [18/30] Train Loss: 0.0424, Validation Loss: 1.0607, Accuracy: 78.32%\n",
      "Epoch [19/30] Train Loss: 0.0563, Validation Loss: 1.0411, Accuracy: 79.05%\n",
      "Epoch [20/30] Train Loss: 0.0435, Validation Loss: 1.1068, Accuracy: 78.64%\n",
      "Epoch [21/30] Train Loss: 0.0371, Validation Loss: 1.0626, Accuracy: 79.60%\n",
      "Epoch [22/30] Train Loss: 0.0427, Validation Loss: 1.1040, Accuracy: 78.44%\n",
      "Epoch [23/30] Train Loss: 0.0390, Validation Loss: 1.0136, Accuracy: 79.99%\n",
      "Saved model with improved validation accuracy: 79.99%\n",
      "Epoch [24/30] Train Loss: 0.0336, Validation Loss: 1.1035, Accuracy: 79.33%\n",
      "Epoch [25/30] Train Loss: 0.0436, Validation Loss: 1.1286, Accuracy: 77.57%\n",
      "Epoch [26/30] Train Loss: 0.0364, Validation Loss: 0.9966, Accuracy: 79.39%\n",
      "Epoch [27/30] Train Loss: 0.0355, Validation Loss: 1.0494, Accuracy: 79.47%\n",
      "Epoch [28/30] Train Loss: 0.0330, Validation Loss: 1.0408, Accuracy: 80.15%\n",
      "Saved model with improved validation accuracy: 80.15%\n",
      "Epoch [29/30] Train Loss: 0.0350, Validation Loss: 1.1997, Accuracy: 77.11%\n",
      "Epoch [30/30] Train Loss: 0.0359, Validation Loss: 1.0136, Accuracy: 79.44%\n",
      "\n",
      "Running experiment with 25% soft labels\n",
      "\n",
      "Training ResNet on CIFAR-10H...\n",
      "Using device: mps\n",
      "Epoch [1/30] Train Loss: 0.9817, Validation Loss: 0.8069, Accuracy: 72.32%\n",
      "Saved model with improved validation accuracy: 72.32%\n",
      "Epoch [2/30] Train Loss: 0.6253, Validation Loss: 0.8053, Accuracy: 73.63%\n",
      "Saved model with improved validation accuracy: 73.63%\n",
      "Epoch [3/30] Train Loss: 0.4741, Validation Loss: 0.6058, Accuracy: 79.39%\n",
      "Saved model with improved validation accuracy: 79.39%\n",
      "Epoch [4/30] Train Loss: 0.3700, Validation Loss: 0.6795, Accuracy: 78.32%\n",
      "Epoch [5/30] Train Loss: 0.2828, Validation Loss: 0.7370, Accuracy: 77.96%\n",
      "Epoch [6/30] Train Loss: 0.2186, Validation Loss: 0.7856, Accuracy: 78.36%\n",
      "Epoch [7/30] Train Loss: 0.1828, Validation Loss: 0.7062, Accuracy: 79.80%\n",
      "Saved model with improved validation accuracy: 79.80%\n",
      "Epoch [8/30] Train Loss: 0.1498, Validation Loss: 0.7232, Accuracy: 79.41%\n",
      "Epoch [9/30] Train Loss: 0.1263, Validation Loss: 0.7386, Accuracy: 79.89%\n",
      "Saved model with improved validation accuracy: 79.89%\n",
      "Epoch [10/30] Train Loss: 0.1275, Validation Loss: 0.8552, Accuracy: 77.41%\n",
      "Epoch [11/30] Train Loss: 0.1045, Validation Loss: 0.7461, Accuracy: 80.37%\n",
      "Saved model with improved validation accuracy: 80.37%\n",
      "Epoch [12/30] Train Loss: 0.0985, Validation Loss: 0.8016, Accuracy: 79.57%\n",
      "Epoch [13/30] Train Loss: 0.0980, Validation Loss: 0.8557, Accuracy: 78.72%\n",
      "Epoch [14/30] Train Loss: 0.0941, Validation Loss: 0.8089, Accuracy: 79.55%\n",
      "Epoch [15/30] Train Loss: 0.0933, Validation Loss: 0.8253, Accuracy: 79.63%\n",
      "Epoch [16/30] Train Loss: 0.0866, Validation Loss: 0.7966, Accuracy: 79.77%\n",
      "Epoch [17/30] Train Loss: 0.0930, Validation Loss: 0.7741, Accuracy: 80.25%\n",
      "Epoch [18/30] Train Loss: 0.0774, Validation Loss: 0.7977, Accuracy: 80.67%\n",
      "Saved model with improved validation accuracy: 80.67%\n",
      "Epoch [19/30] Train Loss: 0.0782, Validation Loss: 0.7654, Accuracy: 80.45%\n",
      "Epoch [20/30] Train Loss: 0.0769, Validation Loss: 0.8735, Accuracy: 78.71%\n",
      "Epoch [21/30] Train Loss: 0.0725, Validation Loss: 0.8225, Accuracy: 80.59%\n",
      "Epoch [22/30] Train Loss: 0.0761, Validation Loss: 0.8610, Accuracy: 79.28%\n",
      "Epoch [23/30] Train Loss: 0.0791, Validation Loss: 1.5190, Accuracy: 77.72%\n",
      "Epoch [24/30] Train Loss: 0.1008, Validation Loss: 0.8067, Accuracy: 80.00%\n",
      "Epoch [25/30] Train Loss: 0.0951, Validation Loss: 0.8355, Accuracy: 79.49%\n",
      "Epoch [26/30] Train Loss: 0.0722, Validation Loss: 0.7954, Accuracy: 80.23%\n",
      "Epoch [27/30] Train Loss: 0.0529, Validation Loss: 0.8066, Accuracy: 81.19%\n",
      "Saved model with improved validation accuracy: 81.19%\n",
      "Epoch [28/30] Train Loss: 0.0585, Validation Loss: 0.8400, Accuracy: 79.85%\n",
      "Epoch [29/30] Train Loss: 0.0585, Validation Loss: 0.7913, Accuracy: 81.12%\n",
      "Epoch [30/30] Train Loss: 0.0660, Validation Loss: 0.8773, Accuracy: 79.63%\n",
      "\n",
      "Running experiment with 50% soft labels\n",
      "\n",
      "Training ResNet on CIFAR-10H...\n",
      "Using device: mps\n",
      "Epoch [1/30] Train Loss: 0.9788, Validation Loss: 0.7323, Accuracy: 75.44%\n",
      "Saved model with improved validation accuracy: 75.44%\n",
      "Epoch [2/30] Train Loss: 0.6286, Validation Loss: 0.7994, Accuracy: 72.51%\n",
      "Epoch [3/30] Train Loss: 0.4846, Validation Loss: 0.6405, Accuracy: 78.79%\n",
      "Saved model with improved validation accuracy: 78.79%\n",
      "Epoch [4/30] Train Loss: 0.3842, Validation Loss: 0.6534, Accuracy: 78.20%\n",
      "Epoch [5/30] Train Loss: 0.3143, Validation Loss: 0.8647, Accuracy: 74.33%\n",
      "Epoch [6/30] Train Loss: 0.2458, Validation Loss: 0.8101, Accuracy: 75.44%\n",
      "Epoch [7/30] Train Loss: 0.2053, Validation Loss: 0.6983, Accuracy: 79.51%\n",
      "Saved model with improved validation accuracy: 79.51%\n",
      "Epoch [8/30] Train Loss: 0.1674, Validation Loss: 0.7136, Accuracy: 80.03%\n",
      "Saved model with improved validation accuracy: 80.03%\n",
      "Epoch [9/30] Train Loss: 0.1602, Validation Loss: 0.7723, Accuracy: 78.29%\n",
      "Epoch [10/30] Train Loss: 0.1471, Validation Loss: 0.8079, Accuracy: 78.21%\n",
      "Epoch [11/30] Train Loss: 0.1334, Validation Loss: 0.8216, Accuracy: 77.88%\n",
      "Epoch [12/30] Train Loss: 0.1304, Validation Loss: 0.9662, Accuracy: 75.44%\n",
      "Epoch [13/30] Train Loss: 0.1484, Validation Loss: 0.8439, Accuracy: 78.12%\n",
      "Epoch [14/30] Train Loss: 0.1130, Validation Loss: 0.8702, Accuracy: 78.87%\n",
      "Epoch [15/30] Train Loss: 0.1127, Validation Loss: 0.7715, Accuracy: 79.76%\n",
      "Epoch [16/30] Train Loss: 0.1169, Validation Loss: 0.8076, Accuracy: 78.97%\n",
      "Epoch [17/30] Train Loss: 0.1183, Validation Loss: 0.7676, Accuracy: 79.84%\n",
      "Epoch [18/30] Train Loss: 0.1055, Validation Loss: 0.8710, Accuracy: 77.87%\n",
      "Epoch [19/30] Train Loss: 0.1063, Validation Loss: 0.7456, Accuracy: 80.11%\n",
      "Saved model with improved validation accuracy: 80.11%\n",
      "Epoch [20/30] Train Loss: 0.1183, Validation Loss: 1.0860, Accuracy: 71.11%\n",
      "Epoch [21/30] Train Loss: 0.1867, Validation Loss: 0.7270, Accuracy: 80.48%\n",
      "Saved model with improved validation accuracy: 80.48%\n",
      "Epoch [22/30] Train Loss: 0.0969, Validation Loss: 0.7854, Accuracy: 79.76%\n",
      "Epoch [23/30] Train Loss: 0.0801, Validation Loss: 0.7369, Accuracy: 81.41%\n",
      "Saved model with improved validation accuracy: 81.41%\n",
      "Epoch [24/30] Train Loss: 0.0826, Validation Loss: 0.7868, Accuracy: 80.32%\n",
      "Epoch [25/30] Train Loss: 0.0859, Validation Loss: 0.8149, Accuracy: 79.67%\n",
      "Epoch [26/30] Train Loss: 0.0979, Validation Loss: 0.8115, Accuracy: 79.49%\n",
      "Epoch [27/30] Train Loss: 0.1114, Validation Loss: 0.8477, Accuracy: 78.69%\n",
      "Epoch [28/30] Train Loss: 0.1025, Validation Loss: 0.8349, Accuracy: 79.08%\n",
      "Epoch [29/30] Train Loss: 0.0956, Validation Loss: 0.8465, Accuracy: 78.76%\n",
      "Epoch [30/30] Train Loss: 0.0886, Validation Loss: 0.8514, Accuracy: 79.56%\n",
      "\n",
      "Running experiment with 75% soft labels\n",
      "\n",
      "Training ResNet on CIFAR-10H...\n",
      "Using device: mps\n",
      "Epoch [1/30] Train Loss: 0.9719, Validation Loss: 0.8730, Accuracy: 70.36%\n",
      "Saved model with improved validation accuracy: 70.36%\n",
      "Epoch [2/30] Train Loss: 0.6596, Validation Loss: 0.9365, Accuracy: 69.13%\n",
      "Epoch [3/30] Train Loss: 0.5016, Validation Loss: 0.7284, Accuracy: 75.80%\n",
      "Saved model with improved validation accuracy: 75.80%\n",
      "Epoch [4/30] Train Loss: 0.4029, Validation Loss: 0.7336, Accuracy: 75.92%\n",
      "Saved model with improved validation accuracy: 75.92%\n",
      "Epoch [5/30] Train Loss: 0.3294, Validation Loss: 0.7421, Accuracy: 76.72%\n",
      "Saved model with improved validation accuracy: 76.72%\n",
      "Epoch [6/30] Train Loss: 0.2722, Validation Loss: 0.6591, Accuracy: 79.79%\n",
      "Saved model with improved validation accuracy: 79.79%\n",
      "Epoch [7/30] Train Loss: 0.2254, Validation Loss: 0.6777, Accuracy: 80.24%\n",
      "Saved model with improved validation accuracy: 80.24%\n",
      "Epoch [8/30] Train Loss: 0.2044, Validation Loss: 0.6994, Accuracy: 79.40%\n",
      "Epoch [9/30] Train Loss: 0.1852, Validation Loss: 0.7271, Accuracy: 79.09%\n",
      "Epoch [10/30] Train Loss: 0.1796, Validation Loss: 0.7484, Accuracy: 78.95%\n",
      "Epoch [11/30] Train Loss: 0.1611, Validation Loss: 0.7975, Accuracy: 78.35%\n",
      "Epoch [12/30] Train Loss: 0.1540, Validation Loss: 0.7240, Accuracy: 79.89%\n",
      "Epoch [13/30] Train Loss: 0.1471, Validation Loss: 0.7604, Accuracy: 79.20%\n",
      "Epoch [14/30] Train Loss: 0.1452, Validation Loss: 0.7810, Accuracy: 79.53%\n",
      "Epoch [15/30] Train Loss: 0.1494, Validation Loss: 0.7200, Accuracy: 80.20%\n",
      "Epoch [16/30] Train Loss: 0.1417, Validation Loss: 0.7181, Accuracy: 80.15%\n",
      "Epoch [17/30] Train Loss: 0.1351, Validation Loss: 0.7875, Accuracy: 78.99%\n",
      "Epoch [18/30] Train Loss: 0.1405, Validation Loss: 0.7514, Accuracy: 79.64%\n",
      "Epoch [19/30] Train Loss: 0.1375, Validation Loss: 0.7368, Accuracy: 80.43%\n",
      "Saved model with improved validation accuracy: 80.43%\n",
      "Epoch [20/30] Train Loss: 0.1371, Validation Loss: 0.7139, Accuracy: 80.23%\n",
      "Epoch [21/30] Train Loss: 0.1438, Validation Loss: 0.8560, Accuracy: 77.48%\n",
      "Epoch [22/30] Train Loss: 0.1283, Validation Loss: 0.8476, Accuracy: 78.04%\n",
      "Epoch [23/30] Train Loss: 0.1245, Validation Loss: 0.8564, Accuracy: 77.91%\n",
      "Epoch [24/30] Train Loss: 0.1435, Validation Loss: 0.7563, Accuracy: 80.71%\n",
      "Saved model with improved validation accuracy: 80.71%\n",
      "Epoch [25/30] Train Loss: 0.1221, Validation Loss: 0.7646, Accuracy: 80.24%\n",
      "Epoch [26/30] Train Loss: 0.1168, Validation Loss: 0.8257, Accuracy: 79.00%\n",
      "Epoch [27/30] Train Loss: 0.1212, Validation Loss: 0.7610, Accuracy: 80.36%\n",
      "Epoch [28/30] Train Loss: 0.1217, Validation Loss: 0.7798, Accuracy: 80.31%\n",
      "Epoch [29/30] Train Loss: 0.1226, Validation Loss: 0.7641, Accuracy: 79.87%\n",
      "Epoch [30/30] Train Loss: 0.1216, Validation Loss: 0.7895, Accuracy: 79.73%\n",
      "\n",
      "Running experiment with 100% soft labels\n",
      "\n",
      "Training ResNet on CIFAR-10H...\n",
      "Using device: mps\n",
      "Epoch [1/30] Train Loss: 0.9885, Validation Loss: 0.8221, Accuracy: 72.43%\n",
      "Saved model with improved validation accuracy: 72.43%\n",
      "Epoch [2/30] Train Loss: 0.6589, Validation Loss: 0.7048, Accuracy: 76.97%\n",
      "Saved model with improved validation accuracy: 76.97%\n",
      "Epoch [3/30] Train Loss: 0.5445, Validation Loss: 0.6970, Accuracy: 76.59%\n",
      "Epoch [4/30] Train Loss: 0.4220, Validation Loss: 0.7410, Accuracy: 76.04%\n",
      "Epoch [5/30] Train Loss: 0.3443, Validation Loss: 0.6466, Accuracy: 79.20%\n",
      "Saved model with improved validation accuracy: 79.20%\n",
      "Epoch [6/30] Train Loss: 0.2918, Validation Loss: 0.6736, Accuracy: 79.23%\n",
      "Saved model with improved validation accuracy: 79.23%\n",
      "Epoch [7/30] Train Loss: 0.2669, Validation Loss: 0.7243, Accuracy: 78.47%\n",
      "Epoch [8/30] Train Loss: 0.2332, Validation Loss: 0.7345, Accuracy: 78.67%\n",
      "Epoch [9/30] Train Loss: 0.2058, Validation Loss: 0.8676, Accuracy: 76.28%\n",
      "Epoch [10/30] Train Loss: 0.1910, Validation Loss: 0.6927, Accuracy: 80.15%\n",
      "Saved model with improved validation accuracy: 80.15%\n",
      "Epoch [11/30] Train Loss: 0.1831, Validation Loss: 0.7434, Accuracy: 78.64%\n",
      "Epoch [12/30] Train Loss: 0.2027, Validation Loss: 0.8158, Accuracy: 75.99%\n",
      "Epoch [13/30] Train Loss: 0.1983, Validation Loss: 0.7418, Accuracy: 78.61%\n",
      "Epoch [14/30] Train Loss: 0.1656, Validation Loss: 0.7682, Accuracy: 78.35%\n",
      "Epoch [15/30] Train Loss: 0.1641, Validation Loss: 0.7506, Accuracy: 78.77%\n",
      "Epoch [16/30] Train Loss: 0.1575, Validation Loss: 0.7188, Accuracy: 80.52%\n",
      "Saved model with improved validation accuracy: 80.52%\n",
      "Epoch [17/30] Train Loss: 0.1643, Validation Loss: 0.8073, Accuracy: 77.72%\n",
      "Epoch [18/30] Train Loss: 0.1657, Validation Loss: 0.8040, Accuracy: 78.00%\n",
      "Epoch [19/30] Train Loss: 0.1607, Validation Loss: 0.8214, Accuracy: 77.89%\n",
      "Epoch [20/30] Train Loss: 0.1563, Validation Loss: 0.7558, Accuracy: 79.36%\n",
      "Epoch [21/30] Train Loss: 0.1564, Validation Loss: 0.7589, Accuracy: 79.63%\n",
      "Epoch [22/30] Train Loss: 0.1535, Validation Loss: 0.8206, Accuracy: 78.32%\n",
      "Epoch [23/30] Train Loss: 0.1612, Validation Loss: 0.7170, Accuracy: 80.16%\n",
      "Epoch [24/30] Train Loss: 0.1472, Validation Loss: 0.7483, Accuracy: 79.57%\n",
      "Epoch [25/30] Train Loss: 0.1502, Validation Loss: 0.7103, Accuracy: 80.41%\n",
      "Epoch [26/30] Train Loss: 0.1532, Validation Loss: 0.8171, Accuracy: 78.37%\n",
      "Epoch [27/30] Train Loss: 0.1518, Validation Loss: 0.8084, Accuracy: 78.57%\n",
      "Epoch [28/30] Train Loss: 0.1434, Validation Loss: 0.8494, Accuracy: 77.69%\n",
      "Epoch [29/30] Train Loss: 0.1404, Validation Loss: 0.7332, Accuracy: 80.43%\n",
      "Epoch [30/30] Train Loss: 0.2635, Validation Loss: 0.7602, Accuracy: 78.53%\n",
      "\n",
      "Ablation Study Results\n",
      "=====================\n",
      "    Soft Labels  train_accuracy    val_accuracy      train_loss        val_loss train_precision   val_precision    train_recall      val_recall        train_f1          val_f1\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "        100%        0.988        0.805        0.053        0.719        0.988        0.808        0.988        0.805        0.988        0.803\n",
      "          0%        0.991        0.801        0.028        1.041        0.991        0.804        0.991        0.801        0.991        0.801\n",
      "         25%        0.997        0.812        0.016        0.807        0.997        0.813        0.997        0.812        0.997        0.811\n",
      "         50%        0.998        0.814        0.017        0.737        0.998        0.814        0.998        0.814        0.998        0.814\n",
      "         75%        0.989        0.807        0.041        0.756        0.989        0.812        0.989        0.807        0.989        0.808\n",
      "        100%        0.988        0.805        0.053        0.719        0.988        0.808        0.988        0.805        0.988        0.803\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    \"\"\"\n",
    "    Evaluate model performance with multiple metrics.\n",
    "    Returns dict with accuracy, precision, recall, f1 score and loss.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            if len(labels.shape) > 1:  # If labels are one-hot encoded\n",
    "                _, labels = torch.max(labels, 1)  # Convert to class indices\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = correct / total\n",
    "    precision = precision_score(all_labels, all_preds, average='macro')\n",
    "    recall = recall_score(all_labels, all_preds, average='macro')\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'loss': avg_loss\n",
    "    }\n",
    "\n",
    "def run_proportion_experiment(\n",
    "    full_dataset,\n",
    "    soft_label_model,\n",
    "    val_loader, \n",
    "    test_loader,\n",
    "    soft_proportions=[0.0, 0.25, 0.5, 0.75, 1.0],\n",
    "    num_epochs=20,\n",
    "    device=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Run experiments with different proportions of soft vs hard labels.\n",
    "    \n",
    "    Args:\n",
    "        full_dataset: Base dataset with hard labels\n",
    "        model: Model to generate soft labels\n",
    "        soft_proportions: List of proportions of soft labels to use\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    results = {prop: {} for prop in soft_proportions}\n",
    "    total_samples = len(full_dataset)\n",
    "    \n",
    "    for prop in soft_proportions:\n",
    "        print(f\"\\nRunning experiment with {int(prop*100)}% soft labels\")\n",
    "        model_path = f\"models/ResNet_cifar10h_soft_{int(prop*100)}percent.pth\"\n",
    "        \n",
    "        # Randomly shuffle the dataset\n",
    "        indices = torch.randperm(total_samples, generator=torch.Generator().manual_seed(42))\n",
    "        \n",
    "        # Calculate size for soft labels\n",
    "        soft_size = int(total_samples * prop)\n",
    "        \n",
    "        # Create soft and hard label datasets\n",
    "        soft_indices = indices[:soft_size]\n",
    "        hard_indices = indices[soft_size:]\n",
    "        \n",
    "        # Create soft label subset\n",
    "        if soft_size > 0:\n",
    "            soft_subset = torch.utils.data.Subset(full_dataset, soft_indices)\n",
    "            soft_loader = DataLoader(soft_subset, batch_size=128, shuffle=False)\n",
    "            soft_dataset = create_soft_label_dataset(soft_label_model, soft_loader, device)\n",
    "        \n",
    "        # Create hard label subset\n",
    "        if len(hard_indices) > 0:\n",
    "            hard_subset = torch.utils.data.Subset(full_dataset, hard_indices)\n",
    "            hard_dataset = CIFAR10LabelDataset(hard_subset)\n",
    "        \n",
    "        # Combine datasets\n",
    "        if prop == 0.0:\n",
    "            combined_dataset = hard_dataset\n",
    "        elif prop == 1.0:\n",
    "            combined_dataset = soft_dataset\n",
    "        else:\n",
    "            combined_dataset = ConcatDataset([hard_dataset, soft_dataset])\n",
    "        \n",
    "        train_loader = DataLoader(combined_dataset, batch_size=128, shuffle=True)\n",
    "        \n",
    "        # Train model\n",
    "        model = models.resnet34(weights=models.ResNet34_Weights.DEFAULT)\n",
    "        train_nn_model(model, train_loader, val_loader, num_epochs=num_epochs, model_path=model_path)\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        model.load_state_dict(torch.load(model_path, weights_only=True))\n",
    "        model.eval()\n",
    "        \n",
    "        test_metrics = evaluate_model(model, test_loader, device)\n",
    "        val_metrics = evaluate_model(model, val_loader, device)\n",
    "        train_metrics_final = evaluate_model(model, train_loader, device)\n",
    "        \n",
    "        results[prop] = {\n",
    "            'train_accuracy': train_metrics_final['accuracy'],\n",
    "            'val_accuracy': val_metrics['accuracy'], \n",
    "            'test_accuracy': test_metrics['accuracy'],\n",
    "            'train_loss': train_metrics_final['loss'],\n",
    "            'val_loss': val_metrics['loss'],\n",
    "            'train_precision': train_metrics_final['precision'],\n",
    "            'val_precision': val_metrics['precision'],\n",
    "            'train_recall': train_metrics_final['recall'],\n",
    "            'val_recall': val_metrics['recall'],\n",
    "            'train_f1': train_metrics_final['f1'],\n",
    "            'val_f1': val_metrics['f1']\n",
    "        }\n",
    "            \n",
    "    # Print results in a paper-friendly format\n",
    "    print(\"\\nAblation Study Results\")\n",
    "    print(\"=====================\")\n",
    "    metrics = ['train_accuracy', 'val_accuracy', 'train_loss', 'val_loss', \n",
    "              'train_precision', 'val_precision', 'train_recall', 'val_recall',\n",
    "              'train_f1', 'val_f1']\n",
    "    \n",
    "    header = f\"{'Soft Labels':>15}\"\n",
    "    for metric in metrics:\n",
    "        header += f\" {metric:>15}\"\n",
    "    print(header)\n",
    "    print(\"-\" * (18 + 18 * len(metrics)))\n",
    "    \n",
    "    for prop in soft_proportions:\n",
    "        line = f\"{prop*100:>11.0f}%\"\n",
    "        for metric in metrics:\n",
    "            line += f\" {results[prop][metric]:>12.3f}\"\n",
    "        print(line)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run the experiment\n",
    "results = run_proportion_experiment(\n",
    "    full_dataset=cifar10_hard_augment_dataset,\n",
    "    soft_label_model=soft_label_model,\n",
    "    val_loader=cifar10_hard_val_loader,\n",
    "    test_loader=cifar10_hard_test_loader,\n",
    "    soft_proportions=[1.0, 0.0, 0.25, 0.5, 0.75, 1.0],\n",
    "    num_epochs=30,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results by Soft Label Percentage:\n",
      "================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>train_precision</th>\n",
       "      <th>val_precision</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>val_recall</th>\n",
       "      <th>train_f1</th>\n",
       "      <th>val_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100%</th>\n",
       "      <td>98.762857</td>\n",
       "      <td>80.520000</td>\n",
       "      <td>80.333333</td>\n",
       "      <td>5.283633</td>\n",
       "      <td>71.878215</td>\n",
       "      <td>98.779932</td>\n",
       "      <td>80.826060</td>\n",
       "      <td>98.767710</td>\n",
       "      <td>80.455698</td>\n",
       "      <td>98.758801</td>\n",
       "      <td>80.274028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0%</th>\n",
       "      <td>99.051429</td>\n",
       "      <td>80.146667</td>\n",
       "      <td>79.906667</td>\n",
       "      <td>2.815941</td>\n",
       "      <td>104.079610</td>\n",
       "      <td>99.058741</td>\n",
       "      <td>80.401502</td>\n",
       "      <td>99.050684</td>\n",
       "      <td>80.141505</td>\n",
       "      <td>99.051164</td>\n",
       "      <td>80.149237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>99.651429</td>\n",
       "      <td>81.186667</td>\n",
       "      <td>80.893333</td>\n",
       "      <td>1.601000</td>\n",
       "      <td>80.658163</td>\n",
       "      <td>99.652127</td>\n",
       "      <td>81.264641</td>\n",
       "      <td>99.651832</td>\n",
       "      <td>81.162617</td>\n",
       "      <td>99.651496</td>\n",
       "      <td>81.096214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>99.762857</td>\n",
       "      <td>81.413333</td>\n",
       "      <td>80.853333</td>\n",
       "      <td>1.665450</td>\n",
       "      <td>73.689637</td>\n",
       "      <td>99.763687</td>\n",
       "      <td>81.414592</td>\n",
       "      <td>99.763294</td>\n",
       "      <td>81.449038</td>\n",
       "      <td>99.763167</td>\n",
       "      <td>81.371133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>98.940000</td>\n",
       "      <td>80.706667</td>\n",
       "      <td>79.706667</td>\n",
       "      <td>4.148401</td>\n",
       "      <td>75.631304</td>\n",
       "      <td>98.947898</td>\n",
       "      <td>81.162575</td>\n",
       "      <td>98.944926</td>\n",
       "      <td>80.694255</td>\n",
       "      <td>98.941458</td>\n",
       "      <td>80.778979</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      train_accuracy  val_accuracy  test_accuracy  train_loss    val_loss  \\\n",
       "100%       98.762857     80.520000      80.333333    5.283633   71.878215   \n",
       "0%         99.051429     80.146667      79.906667    2.815941  104.079610   \n",
       "25%        99.651429     81.186667      80.893333    1.601000   80.658163   \n",
       "50%        99.762857     81.413333      80.853333    1.665450   73.689637   \n",
       "75%        98.940000     80.706667      79.706667    4.148401   75.631304   \n",
       "\n",
       "      train_precision  val_precision  train_recall  val_recall   train_f1  \\\n",
       "100%        98.779932      80.826060     98.767710   80.455698  98.758801   \n",
       "0%          99.058741      80.401502     99.050684   80.141505  99.051164   \n",
       "25%         99.652127      81.264641     99.651832   81.162617  99.651496   \n",
       "50%         99.763687      81.414592     99.763294   81.449038  99.763167   \n",
       "75%         98.947898      81.162575     98.944926   80.694255  98.941458   \n",
       "\n",
       "         val_f1  \n",
       "100%  80.274028  \n",
       "0%    80.149237  \n",
       "25%   81.096214  \n",
       "50%   81.371133  \n",
       "75%   80.778979  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert results to DataFrame for pretty printing\n",
    "df = pd.DataFrame(results).T * 100  # Convert proportions to percentages\n",
    "df.index = [f\"{idx:.0f}%\" for idx in df.index * 100]  # Format index as percentages\n",
    "print(\"\\nResults by Soft Label Percentage:\")\n",
    "print(\"================================\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs229_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
