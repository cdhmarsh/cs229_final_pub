{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This file contains the code to run experiments with artificial soft labels.\n",
    "\n",
    "The experiment is:\n",
    "    * Train a soft label predictor model on CIFAR-10H\n",
    "    * Generate artificial soft labels for CIFAR-10\n",
    "    * Train a model on CIFAR-10 with the artificial soft labels + CIFAR-10H\n",
    "    * Evaluate the model on CIFAR-10\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from torchvision import datasets, transforms, models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from typing import Tuple\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-10 dataset and return augment, train, validation, and test DataLoaders\n",
    "def load_cifar10_experiment():\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.ConvertImageDtype(torch.float32),\n",
    "        ]\n",
    "    )\n",
    "    full_dataset = datasets.CIFAR10(root=\"../data/cifar-10\", train=True, download=True, transform=transform)\n",
    "    # we use the test dataset for training, similar to the CIFAR-10H experiment\n",
    "    train_dataset = datasets.CIFAR10(root=\"../data/cifar-10\", train=False, download=True, transform=transform)\n",
    "\n",
    "    # This dataset will be used for augmenting, testing, and validation.\n",
    "    augment_size = int(0.7 * len(full_dataset))\n",
    "    val_size = (len(full_dataset) - augment_size) // 2\n",
    "    test_size = len(full_dataset) - augment_size - val_size\n",
    "    augment_dataset, test_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        full_dataset, [augment_size, test_size, val_size], generator=torch.Generator().manual_seed(229)\n",
    "    )\n",
    "\n",
    "    return augment_dataset, train_dataset, test_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "CIFAR-10 dataset loaded with 35000 augment, 10000 training, 7500 test, and 7500 validation samples\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    cifar10_hard_augment_dataset,\n",
    "    cifar10_hard_train_dataset,\n",
    "    cifar10_hard_test_dataset,\n",
    "    cifar10_hard_val_dataset,\n",
    ") = load_cifar10_experiment()\n",
    "\n",
    "cifar10_hard_test_loader = DataLoader(cifar10_hard_test_dataset, batch_size=128, shuffle=False)\n",
    "cifar10_hard_val_loader = DataLoader(cifar10_hard_val_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "print(\n",
    "    f\"CIFAR-10 dataset loaded with {len(cifar10_hard_augment_dataset)} augment, {len(cifar10_hard_train_dataset)} training, {len(cifar10_hard_test_dataset)} test, and {len(cifar10_hard_val_dataset)} validation samples\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "Training is done on the CIFAR-10H dataset. Evaluation is done on the CIFAR-10 train set, which we use as a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    optimizer: optim.Optimizer,\n",
    "    num_epochs: int,\n",
    "    model_path,\n",
    ") -> nn.Module:\n",
    "    device = torch.device(\n",
    "        \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    )\n",
    "    print(f\"Using device: {device}\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(images)\n",
    "\n",
    "                if len(labels.shape) > 1:  # For soft labels\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    _, labels = torch.max(labels, 1)\n",
    "                else:  # For hard labels\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                val_loss += criterion(outputs, labels).item()\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        print(\n",
    "            f\"Epoch [{epoch+1}/{num_epochs}] Train Loss: {running_loss/len(train_loader):.4f}, Validation Loss: {val_loss:.4f}, Accuracy: {accuracy:.2f}%\"\n",
    "        )\n",
    "\n",
    "        # Save model if validation accuracy improves\n",
    "        if model_path is not None:\n",
    "            if accuracy > best_val_acc:\n",
    "                best_val_acc = accuracy\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "                print(f\"Saved model with improved validation accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn_model(model, cifar10h_loader, cifar10_val_loader, num_epochs=20, model_path=None):\n",
    "    print(f\"\\nTraining {model.__class__.__name__} on CIFAR-10H...\")\n",
    "\n",
    "    # Adjust the final layer for CIFAR-10\n",
    "    if isinstance(model, models.ResNet):\n",
    "        num_ftrs = model.fc.in_features\n",
    "        model.fc = nn.Linear(num_ftrs, 10)\n",
    "    elif isinstance(model, models.VGG):\n",
    "        num_ftrs = model.classifier[-1].in_features\n",
    "        model.classifier[-1] = nn.Linear(num_ftrs, 10)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    model = train_model(\n",
    "        model=model,\n",
    "        train_loader=cifar10h_loader,\n",
    "        val_loader=cifar10_val_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        num_epochs=num_epochs,\n",
    "        model_path=model_path\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageHardToSoftLabelModel(\n",
       "  (image_encoder): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU()\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fc_image): Linear(in_features=4096, out_features=128, bias=True)\n",
       "  (fc_label): Linear(in_features=10, out_features=128, bias=True)\n",
       "  (fc_output): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Dropout(p=0.2, inplace=False)\n",
       "    (4): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): Dropout(p=0.2, inplace=False)\n",
       "    (8): Linear(in_features=64, out_features=10, bias=True)\n",
       "    (9): Softmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from generate_soft_labels import create_soft_label_dataloader, create_soft_label_dataset\n",
    "from soft_label_predictor import ImageHardToSoftLabelModel\n",
    "\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "# Load the trained model\n",
    "soft_label_model = ImageHardToSoftLabelModel().to(device)\n",
    "soft_label_model.load_state_dict(torch.load(\"models/soft_label_model.pt\", weights_only=True))\n",
    "soft_label_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new class to handle both hard and soft labels consistently\n",
    "class CIFAR10LabelDataset(Dataset):\n",
    "    def __init__(self, dataset, soft_labels=None):\n",
    "        self.dataset = dataset\n",
    "        self.soft_labels = soft_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.dataset[idx]\n",
    "        if self.soft_labels is None:\n",
    "            # Convert hard labels to one-hot\n",
    "            label = F.one_hot(torch.tensor(label), num_classes=10).float()\n",
    "        else:\n",
    "            label = torch.tensor(self.soft_labels[idx])\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running experiment with 0% soft labels\n",
      "\n",
      "Training ResNet on CIFAR-10H...\n",
      "Using device: mps\n",
      "Epoch [1/15] Train Loss: 0.8941, Validation Loss: 0.7279, Accuracy: 74.92%\n",
      "Saved model with improved validation accuracy: 74.92%\n",
      "Epoch [2/15] Train Loss: 0.6252, Validation Loss: 0.7842, Accuracy: 74.68%\n",
      "Epoch [3/15] Train Loss: 0.5060, Validation Loss: 0.6152, Accuracy: 78.92%\n",
      "Saved model with improved validation accuracy: 78.92%\n",
      "Epoch [4/15] Train Loss: 0.3661, Validation Loss: 0.6958, Accuracy: 78.05%\n",
      "Epoch [5/15] Train Loss: 0.2708, Validation Loss: 0.8131, Accuracy: 75.93%\n",
      "Epoch [6/15] Train Loss: 0.3249, Validation Loss: 0.6234, Accuracy: 81.55%\n",
      "Saved model with improved validation accuracy: 81.55%\n",
      "Epoch [7/15] Train Loss: 0.1588, Validation Loss: 0.6857, Accuracy: 81.16%\n",
      "Epoch [8/15] Train Loss: 0.1263, Validation Loss: 0.7826, Accuracy: 79.47%\n",
      "Epoch [9/15] Train Loss: 0.1113, Validation Loss: 0.8846, Accuracy: 77.49%\n",
      "Epoch [10/15] Train Loss: 0.1641, Validation Loss: 0.8767, Accuracy: 79.13%\n",
      "Epoch [11/15] Train Loss: 0.0984, Validation Loss: 0.8419, Accuracy: 80.08%\n",
      "Epoch [12/15] Train Loss: 0.0632, Validation Loss: 0.9557, Accuracy: 79.71%\n",
      "Epoch [13/15] Train Loss: 0.0624, Validation Loss: 0.9338, Accuracy: 79.63%\n",
      "Epoch [14/15] Train Loss: 0.0611, Validation Loss: 0.8511, Accuracy: 80.73%\n",
      "Epoch [15/15] Train Loss: 0.0620, Validation Loss: 0.9020, Accuracy: 80.29%\n",
      "\n",
      "Running experiment with 25% soft labels\n",
      "\n",
      "Training ResNet on CIFAR-10H...\n",
      "Using device: mps\n",
      "Epoch [1/15] Train Loss: 0.9383, Validation Loss: 0.7992, Accuracy: 72.44%\n",
      "Saved model with improved validation accuracy: 72.44%\n",
      "Epoch [2/15] Train Loss: 0.6220, Validation Loss: 0.6445, Accuracy: 78.52%\n",
      "Saved model with improved validation accuracy: 78.52%\n",
      "Epoch [3/15] Train Loss: 0.5096, Validation Loss: 0.7759, Accuracy: 74.19%\n",
      "Epoch [4/15] Train Loss: 0.4093, Validation Loss: 0.7252, Accuracy: 76.45%\n",
      "Epoch [5/15] Train Loss: 0.3320, Validation Loss: 0.6674, Accuracy: 78.88%\n",
      "Saved model with improved validation accuracy: 78.88%\n",
      "Epoch [6/15] Train Loss: 0.2579, Validation Loss: 0.6966, Accuracy: 78.57%\n",
      "Epoch [7/15] Train Loss: 0.2268, Validation Loss: 0.6540, Accuracy: 80.55%\n",
      "Saved model with improved validation accuracy: 80.55%\n",
      "Epoch [8/15] Train Loss: 0.1898, Validation Loss: 0.6420, Accuracy: 81.69%\n",
      "Saved model with improved validation accuracy: 81.69%\n",
      "Epoch [9/15] Train Loss: 0.1728, Validation Loss: 0.7105, Accuracy: 79.48%\n",
      "Epoch [10/15] Train Loss: 0.2186, Validation Loss: 0.6737, Accuracy: 80.49%\n",
      "Epoch [11/15] Train Loss: 0.1558, Validation Loss: 0.6767, Accuracy: 81.55%\n",
      "Epoch [12/15] Train Loss: 0.1311, Validation Loss: 0.7633, Accuracy: 79.91%\n",
      "Epoch [13/15] Train Loss: 0.1351, Validation Loss: 0.7482, Accuracy: 79.91%\n",
      "Epoch [14/15] Train Loss: 0.1433, Validation Loss: 0.8463, Accuracy: 78.51%\n",
      "Epoch [15/15] Train Loss: 0.1353, Validation Loss: 0.7052, Accuracy: 81.16%\n",
      "\n",
      "Running experiment with 50% soft labels\n",
      "\n",
      "Training ResNet on CIFAR-10H...\n",
      "Using device: mps\n",
      "Epoch [1/15] Train Loss: 0.9576, Validation Loss: 0.7765, Accuracy: 72.63%\n",
      "Saved model with improved validation accuracy: 72.63%\n",
      "Epoch [2/15] Train Loss: 0.6653, Validation Loss: 0.7151, Accuracy: 76.47%\n",
      "Saved model with improved validation accuracy: 76.47%\n",
      "Epoch [3/15] Train Loss: 0.5382, Validation Loss: 0.7713, Accuracy: 74.93%\n",
      "Epoch [4/15] Train Loss: 0.4504, Validation Loss: 0.7299, Accuracy: 76.36%\n",
      "Epoch [5/15] Train Loss: 0.3814, Validation Loss: 0.6180, Accuracy: 79.92%\n",
      "Saved model with improved validation accuracy: 79.92%\n",
      "Epoch [6/15] Train Loss: 0.3228, Validation Loss: 0.9800, Accuracy: 71.40%\n",
      "Epoch [7/15] Train Loss: 0.3185, Validation Loss: 0.6048, Accuracy: 81.65%\n",
      "Saved model with improved validation accuracy: 81.65%\n",
      "Epoch [8/15] Train Loss: 0.2437, Validation Loss: 0.6918, Accuracy: 79.39%\n",
      "Epoch [9/15] Train Loss: 0.2302, Validation Loss: 0.6813, Accuracy: 79.97%\n",
      "Epoch [10/15] Train Loss: 0.2219, Validation Loss: 0.6423, Accuracy: 81.37%\n",
      "Epoch [11/15] Train Loss: 0.2296, Validation Loss: 1.2792, Accuracy: 70.23%\n",
      "Epoch [12/15] Train Loss: 0.2958, Validation Loss: 0.6802, Accuracy: 80.71%\n",
      "Epoch [13/15] Train Loss: 0.2126, Validation Loss: 0.6539, Accuracy: 81.60%\n",
      "Epoch [14/15] Train Loss: 0.1800, Validation Loss: 0.6523, Accuracy: 81.81%\n",
      "Saved model with improved validation accuracy: 81.81%\n",
      "Epoch [15/15] Train Loss: 0.1882, Validation Loss: 0.7288, Accuracy: 80.07%\n",
      "\n",
      "Running experiment with 75% soft labels\n",
      "\n",
      "Training ResNet on CIFAR-10H...\n",
      "Using device: mps\n",
      "Epoch [1/15] Train Loss: 1.0001, Validation Loss: 0.7808, Accuracy: 73.68%\n",
      "Saved model with improved validation accuracy: 73.68%\n",
      "Epoch [2/15] Train Loss: 0.6919, Validation Loss: 0.9175, Accuracy: 69.92%\n",
      "Epoch [3/15] Train Loss: 0.6285, Validation Loss: 0.6211, Accuracy: 79.23%\n",
      "Saved model with improved validation accuracy: 79.23%\n",
      "Epoch [4/15] Train Loss: 0.4936, Validation Loss: 0.6313, Accuracy: 79.07%\n",
      "Epoch [5/15] Train Loss: 0.4259, Validation Loss: 0.6015, Accuracy: 80.52%\n",
      "Saved model with improved validation accuracy: 80.52%\n",
      "Epoch [6/15] Train Loss: 0.3862, Validation Loss: 0.6176, Accuracy: 80.80%\n",
      "Saved model with improved validation accuracy: 80.80%\n",
      "Epoch [7/15] Train Loss: 0.3405, Validation Loss: 0.6361, Accuracy: 80.28%\n",
      "Epoch [8/15] Train Loss: 0.3152, Validation Loss: 0.6751, Accuracy: 79.79%\n",
      "Epoch [9/15] Train Loss: 0.2978, Validation Loss: 0.6521, Accuracy: 80.59%\n",
      "Epoch [10/15] Train Loss: 0.2837, Validation Loss: 0.6252, Accuracy: 81.12%\n",
      "Saved model with improved validation accuracy: 81.12%\n",
      "Epoch [11/15] Train Loss: 0.2842, Validation Loss: 0.6675, Accuracy: 80.32%\n",
      "Epoch [12/15] Train Loss: 0.2667, Validation Loss: 0.7288, Accuracy: 79.15%\n",
      "Epoch [13/15] Train Loss: 0.2724, Validation Loss: 0.6521, Accuracy: 81.15%\n",
      "Saved model with improved validation accuracy: 81.15%\n",
      "Epoch [14/15] Train Loss: 0.3355, Validation Loss: 0.6618, Accuracy: 80.21%\n",
      "Epoch [15/15] Train Loss: 0.2589, Validation Loss: 0.6852, Accuracy: 80.79%\n",
      "\n",
      "Running experiment with 100% soft labels\n",
      "\n",
      "Training ResNet on CIFAR-10H...\n",
      "Using device: mps\n",
      "Epoch [1/15] Train Loss: 0.9985, Validation Loss: 0.8558, Accuracy: 71.83%\n",
      "Saved model with improved validation accuracy: 71.83%\n",
      "Epoch [2/15] Train Loss: 0.7191, Validation Loss: 0.7514, Accuracy: 75.33%\n",
      "Saved model with improved validation accuracy: 75.33%\n",
      "Epoch [3/15] Train Loss: 0.6083, Validation Loss: 0.7683, Accuracy: 75.15%\n",
      "Epoch [4/15] Train Loss: 0.5261, Validation Loss: 0.7583, Accuracy: 75.57%\n",
      "Saved model with improved validation accuracy: 75.57%\n",
      "Epoch [5/15] Train Loss: 0.4924, Validation Loss: 0.6127, Accuracy: 80.19%\n",
      "Saved model with improved validation accuracy: 80.19%\n",
      "Epoch [6/15] Train Loss: 0.4152, Validation Loss: 0.6330, Accuracy: 80.17%\n",
      "Epoch [7/15] Train Loss: 0.3826, Validation Loss: 0.6597, Accuracy: 79.69%\n",
      "Epoch [8/15] Train Loss: 0.3730, Validation Loss: 0.6365, Accuracy: 80.60%\n",
      "Saved model with improved validation accuracy: 80.60%\n",
      "Epoch [9/15] Train Loss: 0.3474, Validation Loss: 0.6215, Accuracy: 81.11%\n",
      "Saved model with improved validation accuracy: 81.11%\n",
      "Epoch [10/15] Train Loss: 0.3310, Validation Loss: 0.6702, Accuracy: 80.08%\n",
      "Epoch [11/15] Train Loss: 0.3251, Validation Loss: 0.6555, Accuracy: 80.36%\n",
      "Epoch [12/15] Train Loss: 0.3272, Validation Loss: 0.6749, Accuracy: 80.24%\n",
      "Epoch [13/15] Train Loss: 0.3209, Validation Loss: 0.6648, Accuracy: 80.67%\n",
      "Epoch [14/15] Train Loss: 0.3202, Validation Loss: 0.6567, Accuracy: 80.33%\n",
      "Epoch [15/15] Train Loss: 0.3105, Validation Loss: 0.6628, Accuracy: 80.87%\n",
      "\n",
      "Ablation Study Results\n",
      "=====================\n",
      "    Soft Labels  train_accuracy    val_accuracy      train_loss        val_loss train_precision   val_precision    train_recall      val_recall        train_f1          val_f1\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "          0%        0.956        0.815        0.136        0.623        0.957        0.817        0.956        0.816        0.956        0.816\n",
      "         25%        0.971        0.817        0.096        0.642        0.972        0.819        0.971        0.818        0.971        0.817\n",
      "         50%        0.989        0.818        0.062        0.652        0.989        0.820        0.989        0.818        0.989        0.818\n",
      "         75%        0.982        0.811        0.087        0.652        0.982        0.811        0.982        0.811        0.982        0.810\n",
      "        100%        0.971        0.811        0.133        0.621        0.972        0.814        0.971        0.811        0.971        0.811\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    \"\"\"\n",
    "    Evaluate model performance with multiple metrics.\n",
    "    Returns dict with accuracy, precision, recall, f1 score and loss.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            if len(labels.shape) > 1:  # If labels are one-hot encoded\n",
    "                _, labels = torch.max(labels, 1)  # Convert to class indices\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = correct / total\n",
    "    precision = precision_score(all_labels, all_preds, average='macro')\n",
    "    recall = recall_score(all_labels, all_preds, average='macro')\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'loss': avg_loss\n",
    "    }\n",
    "\n",
    "def run_proportion_experiment(\n",
    "    full_dataset,\n",
    "    soft_label_model,\n",
    "    val_loader, \n",
    "    test_loader,\n",
    "    soft_proportions=[0.0, 0.25, 0.5, 0.75, 1.0],\n",
    "    num_epochs=20,\n",
    "    device=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Run experiments with different proportions of soft vs hard labels.\n",
    "    \n",
    "    Args:\n",
    "        full_dataset: Base dataset with hard labels\n",
    "        model: Model to generate soft labels\n",
    "        soft_proportions: List of proportions of soft labels to use\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    results = {prop: {} for prop in soft_proportions}\n",
    "    total_samples = len(full_dataset)\n",
    "    \n",
    "    for prop in soft_proportions:\n",
    "        print(f\"\\nRunning experiment with {int(prop*100)}% soft labels\")\n",
    "        model_path = f\"models/ResNet_cifar10h_soft_{int(prop*100)}percent.pth\"\n",
    "        \n",
    "        # Randomly shuffle the dataset\n",
    "        indices = torch.randperm(total_samples, generator=torch.Generator().manual_seed(42))\n",
    "        \n",
    "        # Calculate size for soft labels\n",
    "        soft_size = int(total_samples * prop)\n",
    "        \n",
    "        # Create soft and hard label datasets\n",
    "        soft_indices = indices[:soft_size]\n",
    "        hard_indices = indices[soft_size:]\n",
    "        \n",
    "        # Create soft label subset\n",
    "        if soft_size > 0:\n",
    "            soft_subset = torch.utils.data.Subset(full_dataset, soft_indices)\n",
    "            soft_loader = DataLoader(soft_subset, batch_size=128, shuffle=False)\n",
    "            soft_dataset = create_soft_label_dataset(soft_label_model, soft_loader, device)\n",
    "        \n",
    "        # Create hard label subset\n",
    "        if len(hard_indices) > 0:\n",
    "            hard_subset = torch.utils.data.Subset(full_dataset, hard_indices)\n",
    "            hard_dataset = CIFAR10LabelDataset(hard_subset)\n",
    "        \n",
    "        # Combine datasets\n",
    "        if prop == 0.0:\n",
    "            combined_dataset = hard_dataset\n",
    "        elif prop == 1.0:\n",
    "            combined_dataset = soft_dataset\n",
    "        else:\n",
    "            combined_dataset = ConcatDataset([hard_dataset, soft_dataset])\n",
    "        \n",
    "        train_loader = DataLoader(combined_dataset, batch_size=128, shuffle=True)\n",
    "        \n",
    "        # Train model\n",
    "        model = models.resnet34(weights=models.ResNet34_Weights.DEFAULT)\n",
    "        train_nn_model(model, train_loader, val_loader, num_epochs=num_epochs, model_path=model_path)\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        model.load_state_dict(torch.load(model_path, weights_only=True))\n",
    "        model.eval()\n",
    "        \n",
    "        test_metrics = evaluate_model(model, test_loader, device)\n",
    "        val_metrics = evaluate_model(model, val_loader, device)\n",
    "        train_metrics_final = evaluate_model(model, train_loader, device)\n",
    "        \n",
    "        results[prop] = {\n",
    "            'train_accuracy': train_metrics_final['accuracy'],\n",
    "            'val_accuracy': val_metrics['accuracy'], \n",
    "            'test_accuracy': test_metrics['accuracy'],\n",
    "            'train_loss': train_metrics_final['loss'],\n",
    "            'val_loss': val_metrics['loss'],\n",
    "            'train_precision': train_metrics_final['precision'],\n",
    "            'val_precision': val_metrics['precision'],\n",
    "            'train_recall': train_metrics_final['recall'],\n",
    "            'val_recall': val_metrics['recall'],\n",
    "            'train_f1': train_metrics_final['f1'],\n",
    "            'val_f1': val_metrics['f1']\n",
    "        }\n",
    "            \n",
    "    # Print results in a paper-friendly format\n",
    "    print(\"\\nAblation Study Results\")\n",
    "    print(\"=====================\")\n",
    "    metrics = ['train_accuracy', 'val_accuracy', 'train_loss', 'val_loss', \n",
    "              'train_precision', 'val_precision', 'train_recall', 'val_recall',\n",
    "              'train_f1', 'val_f1']\n",
    "    \n",
    "    header = f\"{'Soft Labels':>15}\"\n",
    "    for metric in metrics:\n",
    "        header += f\" {metric:>15}\"\n",
    "    print(header)\n",
    "    print(\"-\" * (18 + 18 * len(metrics)))\n",
    "    \n",
    "    for prop in soft_proportions:\n",
    "        line = f\"{prop*100:>11.0f}%\"\n",
    "        for metric in metrics:\n",
    "            line += f\" {results[prop][metric]:>12.3f}\"\n",
    "        print(line)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run the experiment\n",
    "results = run_proportion_experiment(\n",
    "    full_dataset=cifar10_hard_augment_dataset,\n",
    "    soft_label_model=soft_label_model,\n",
    "    val_loader=cifar10_hard_val_loader,\n",
    "    test_loader=cifar10_hard_test_loader,\n",
    "    soft_proportions=[0.0, 0.25, 0.5, 0.75, 1.0],\n",
    "    num_epochs=30,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results by Soft Label Percentage:\n",
      "================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>train_precision</th>\n",
       "      <th>val_precision</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>val_recall</th>\n",
       "      <th>train_f1</th>\n",
       "      <th>val_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0%</th>\n",
       "      <td>95.624444</td>\n",
       "      <td>81.546667</td>\n",
       "      <td>81.800000</td>\n",
       "      <td>13.567844</td>\n",
       "      <td>62.341590</td>\n",
       "      <td>95.668853</td>\n",
       "      <td>81.749234</td>\n",
       "      <td>95.628476</td>\n",
       "      <td>81.593592</td>\n",
       "      <td>95.621848</td>\n",
       "      <td>81.551903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>97.115556</td>\n",
       "      <td>81.693333</td>\n",
       "      <td>81.746667</td>\n",
       "      <td>9.629539</td>\n",
       "      <td>64.203657</td>\n",
       "      <td>97.154248</td>\n",
       "      <td>81.881143</td>\n",
       "      <td>97.117113</td>\n",
       "      <td>81.776473</td>\n",
       "      <td>97.113685</td>\n",
       "      <td>81.686068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>98.875556</td>\n",
       "      <td>81.813333</td>\n",
       "      <td>81.453333</td>\n",
       "      <td>6.219566</td>\n",
       "      <td>65.232165</td>\n",
       "      <td>98.878358</td>\n",
       "      <td>81.987819</td>\n",
       "      <td>98.878021</td>\n",
       "      <td>81.797542</td>\n",
       "      <td>98.876375</td>\n",
       "      <td>81.828006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>98.228889</td>\n",
       "      <td>81.146667</td>\n",
       "      <td>80.946667</td>\n",
       "      <td>8.659347</td>\n",
       "      <td>65.206027</td>\n",
       "      <td>98.232713</td>\n",
       "      <td>81.110986</td>\n",
       "      <td>98.230829</td>\n",
       "      <td>81.134467</td>\n",
       "      <td>98.225896</td>\n",
       "      <td>81.002469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100%</th>\n",
       "      <td>97.122222</td>\n",
       "      <td>81.106667</td>\n",
       "      <td>81.680000</td>\n",
       "      <td>13.287924</td>\n",
       "      <td>62.148807</td>\n",
       "      <td>97.167938</td>\n",
       "      <td>81.420162</td>\n",
       "      <td>97.120084</td>\n",
       "      <td>81.101570</td>\n",
       "      <td>97.120099</td>\n",
       "      <td>81.074146</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      train_accuracy  val_accuracy  test_accuracy  train_loss   val_loss  \\\n",
       "0%         95.624444     81.546667      81.800000   13.567844  62.341590   \n",
       "25%        97.115556     81.693333      81.746667    9.629539  64.203657   \n",
       "50%        98.875556     81.813333      81.453333    6.219566  65.232165   \n",
       "75%        98.228889     81.146667      80.946667    8.659347  65.206027   \n",
       "100%       97.122222     81.106667      81.680000   13.287924  62.148807   \n",
       "\n",
       "      train_precision  val_precision  train_recall  val_recall   train_f1  \\\n",
       "0%          95.668853      81.749234     95.628476   81.593592  95.621848   \n",
       "25%         97.154248      81.881143     97.117113   81.776473  97.113685   \n",
       "50%         98.878358      81.987819     98.878021   81.797542  98.876375   \n",
       "75%         98.232713      81.110986     98.230829   81.134467  98.225896   \n",
       "100%        97.167938      81.420162     97.120084   81.101570  97.120099   \n",
       "\n",
       "         val_f1  \n",
       "0%    81.551903  \n",
       "25%   81.686068  \n",
       "50%   81.828006  \n",
       "75%   81.002469  \n",
       "100%  81.074146  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert results to DataFrame for pretty printing\n",
    "df = pd.DataFrame(results).T * 100  # Convert proportions to percentages\n",
    "df.index = [f\"{idx:.0f}%\" for idx in df.index * 100]  # Format index as percentages\n",
    "print(\"\\nResults by Soft Label Percentage:\")\n",
    "print(\"================================\")\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs229_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
