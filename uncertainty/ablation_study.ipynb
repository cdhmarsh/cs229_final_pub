{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This file contains the code to run experiments with artificial soft labels.\n",
    "\n",
    "The experiment is:\n",
    "    * Train a soft label predictor model on CIFAR-10H\n",
    "    * Generate artificial soft labels for CIFAR-10\n",
    "    * Train a model on CIFAR-10 with the artificial soft labels + CIFAR-10H\n",
    "    * Evaluate the model on CIFAR-10\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset, Subset\n",
    "from torchvision import datasets, transforms, models\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from generate_soft_labels import create_soft_label_dataset\n",
    "from soft_label_predictor import ImageHardToSoftLabelModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device() -> torch.device:\n",
    "    \"\"\"Get the appropriate device (CUDA, MPS, or CPU).\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    return torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cifar10_experiment() -> Tuple[Dataset, Dataset, Dataset, Dataset]:\n",
    "    \"\"\"Load and split CIFAR-10 dataset into augment, train, test and validation sets.\"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.ConvertImageDtype(torch.float32),\n",
    "    ])\n",
    "\n",
    "    full_dataset = datasets.CIFAR10(root=\"../data/cifar-10\", train=True, download=True, transform=transform)\n",
    "    # Use test dataset for training, similar to CIFAR-10H experiment\n",
    "    train_dataset = datasets.CIFAR10(root=\"../data/cifar-10\", train=False, download=True, transform=transform)\n",
    "\n",
    "    # Split full dataset for augmenting, testing, and validation\n",
    "    augment_size = int(0.7 * len(full_dataset))\n",
    "    val_size = (len(full_dataset) - augment_size) // 2\n",
    "    test_size = len(full_dataset) - augment_size - val_size\n",
    "\n",
    "    generator = torch.Generator().manual_seed(229)\n",
    "    augment_dataset, test_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        full_dataset, [augment_size, test_size, val_size], generator=generator\n",
    "    )\n",
    "\n",
    "    return augment_dataset, train_dataset, test_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10LabelDataset(Dataset):\n",
    "    \"\"\"Dataset wrapper that handles both hard and soft labels consistently.\"\"\"\n",
    "    def __init__(self, dataset: Dataset, soft_labels: Optional[np.ndarray] = None):\n",
    "        self.dataset = dataset\n",
    "        self.soft_labels = soft_labels\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        image, label = self.dataset[idx]\n",
    "        if self.soft_labels is None:\n",
    "            # Convert hard labels to one-hot\n",
    "            label = F.one_hot(torch.tensor(label), num_classes=10).float()\n",
    "        else:\n",
    "            label = torch.tensor(self.soft_labels[idx])\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "Training is done on the CIFAR-10H dataset. Evaluation is done on the CIFAR-10 train set, which we use as a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    device: torch.device,\n",
    "    num_epochs: int = 20,\n",
    "    model_path: Optional[str] = None,\n",
    ") -> nn.Module:\n",
    "    \"\"\"Train a neural network model and save the best version based on validation accuracy.\"\"\"\n",
    "    # Adjust the final layer for CIFAR-10\n",
    "    if isinstance(model, models.ResNet):\n",
    "        num_ftrs = model.fc.in_features\n",
    "        model.fc = nn.Linear(num_ftrs, 10)\n",
    "    elif isinstance(model, models.VGG):\n",
    "        num_ftrs = model.classifier[-1].in_features\n",
    "        model.classifier[-1] = nn.Linear(num_ftrs, 10)\n",
    "        \n",
    "    print(f\"Training on {device}\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    best_val_acc = 0.0\n",
    "\n",
    "    print(f\"\\nTraining {model.__class__.__name__}...\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(images)\n",
    "\n",
    "                if len(labels.shape) > 1:  # For soft labels\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    _, labels = torch.max(labels, 1)\n",
    "                else:  # For hard labels\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                val_loss += criterion(outputs, labels).item()\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        print(\n",
    "            f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "            f\"Train Loss: {running_loss/len(train_loader):.4f}, \"\n",
    "            f\"Validation Loss: {val_loss:.4f}, \"\n",
    "            f\"Accuracy: {accuracy:.2f}%\"\n",
    "        )\n",
    "\n",
    "        # Save model if validation accuracy improves\n",
    "        if model_path is not None and accuracy > best_val_acc:\n",
    "            best_val_acc = accuracy\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            print(f\"Saved model with improved validation accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model: nn.Module, dataloader: DataLoader, device: torch.device) -> Dict:\n",
    "    \"\"\"Evaluate model performance with multiple metrics.\"\"\"\n",
    "    model.eval()\n",
    "    criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "    \n",
    "    total = correct = total_loss = total_cross_entropy = total_kl_div = 0\n",
    "    all_preds, all_labels = [], []\n",
    "    all_pred_probs, all_true_probs = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            original_labels = labels.clone()\n",
    "            \n",
    "            if len(labels.shape) > 1:\n",
    "                _, labels = torch.max(labels, 1)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            pred_probs = F.softmax(outputs, dim=1)\n",
    "            log_probs = F.log_softmax(outputs, dim=1)\n",
    "            \n",
    "            # Handle soft vs hard labels\n",
    "            if len(original_labels.shape) > 1:\n",
    "                original_labels = original_labels.to(device)\n",
    "                cross_entropy = -(original_labels * log_probs).sum()\n",
    "                total_cross_entropy += cross_entropy.item()\n",
    "                kl_div = F.kl_div(log_probs, original_labels, reduction='sum')\n",
    "                all_true_probs.extend(original_labels.cpu().numpy())\n",
    "            else:\n",
    "                total_cross_entropy += loss.item()\n",
    "                true_probs = F.one_hot(labels, num_classes=outputs.size(1)).float()\n",
    "                kl_div = F.kl_div(log_probs, true_probs, reduction='sum')\n",
    "                all_true_probs.extend(true_probs.cpu().numpy())\n",
    "            \n",
    "            total_kl_div += kl_div.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_pred_probs.extend(pred_probs.cpu().numpy())\n",
    "\n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'accuracy': correct / total,\n",
    "        'precision': precision_score(all_labels, all_preds, average='macro', zero_division=0),\n",
    "        'recall': recall_score(all_labels, all_preds, average='macro', zero_division=0),\n",
    "        'f1': f1_score(all_labels, all_preds, average='macro', zero_division=0),\n",
    "        'loss': total_loss / total,  # Per-sample loss\n",
    "        'cross_entropy': total_cross_entropy / total,  # Per-sample cross entropy\n",
    "        'kl_divergence': total_kl_div / total,  # Per-sample KL divergence\n",
    "        'confusion_matrix': confusion_matrix(all_labels, all_preds),\n",
    "        'true_labels': np.array(all_labels),\n",
    "        'predictions': np.array(all_preds),\n",
    "        'pred_probabilities': np.array(all_pred_probs),\n",
    "        'true_probabilities': np.array(all_true_probs)\n",
    "    }\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_proportion_experiment(\n",
    "    full_dataset: Dataset,\n",
    "    soft_label_model: nn.Module,\n",
    "    val_loader: DataLoader,\n",
    "    test_loader: DataLoader,\n",
    "    device: torch.device,\n",
    "    soft_proportions: List[float] = [0.0, 0.25, 0.5, 0.75, 1.0],\n",
    "    num_epochs: int = 20,\n",
    ") -> Dict:\n",
    "    \"\"\"Run experiments with different proportions of soft vs hard labels.\"\"\"    \n",
    "    results = {}\n",
    "    total_samples = len(full_dataset)\n",
    "    \n",
    "    for prop in soft_proportions:\n",
    "        print(f\"\\nRunning experiment with {int(prop*100)}% soft labels\")\n",
    "        model_path = f\"models/ResNet_cifar10h_soft_{int(prop*100)}percent.pth\"\n",
    "        \n",
    "        # Split dataset into soft and hard label portions\n",
    "        indices = torch.randperm(total_samples, generator=torch.Generator().manual_seed(42))\n",
    "        soft_size = int(total_samples * prop)\n",
    "        \n",
    "        soft_indices = indices[:soft_size]\n",
    "        hard_indices = indices[soft_size:]\n",
    "        \n",
    "        # Create datasets\n",
    "        if soft_size > 0:\n",
    "            soft_subset = Subset(full_dataset, soft_indices)\n",
    "            soft_loader = DataLoader(soft_subset, batch_size=128, shuffle=False)\n",
    "            soft_dataset = create_soft_label_dataset(soft_label_model, soft_loader, device)\n",
    "        \n",
    "        if len(hard_indices) > 0:\n",
    "            hard_subset = Subset(full_dataset, hard_indices)\n",
    "            hard_dataset = CIFAR10LabelDataset(hard_subset)\n",
    "        \n",
    "        # Combine datasets based on proportion\n",
    "        combined_dataset = (\n",
    "            hard_dataset if prop == 0.0\n",
    "            else soft_dataset if prop == 1.0\n",
    "            else ConcatDataset([hard_dataset, soft_dataset])\n",
    "        )\n",
    "        \n",
    "        train_loader = DataLoader(combined_dataset, batch_size=128, shuffle=True)\n",
    "        \n",
    "        # Train and evaluate model\n",
    "        model = models.resnet34(weights=models.ResNet34_Weights.DEFAULT)\n",
    "        train_model(model, train_loader, val_loader, num_epochs=num_epochs, model_path=model_path, device=device)\n",
    "        \n",
    "        model.load_state_dict(torch.load(model_path, weights_only=True))\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "        \n",
    "        # Collect metrics\n",
    "        results[prop] = {\n",
    "            **{f'train_{k}': v for k, v in evaluate_model(model, train_loader, device).items()},\n",
    "            **{f'val_{k}': v for k, v in evaluate_model(model, val_loader, device).items()},\n",
    "            **{f'test_{k}': v for k, v in evaluate_model(model, test_loader, device).items()}\n",
    "        }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_basic_metrics(results: Dict):\n",
    "    \"\"\"Plot basic metrics like accuracy, loss etc.\"\"\"\n",
    "    metrics = [\n",
    "        ('cross_entropy', 'Cross Entropy Loss'),\n",
    "        ('kl_divergence', 'KL Divergence'),\n",
    "        ('accuracy', 'Accuracy'),\n",
    "        ('precision', 'Precision'),\n",
    "        ('recall', 'Recall'),\n",
    "        ('f1', 'F1 Score')\n",
    "    ]\n",
    "\n",
    "    proportions = list(results.keys())\n",
    "    x_axis = [p*100 for p in proportions]\n",
    "\n",
    "    fig, axes = plt.subplots(3, 2, figsize=(15, 18))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, (metric, title) in enumerate(metrics):\n",
    "        train_metric = [results[p][f'train_{metric}'] * 100 for p in proportions]\n",
    "        val_metric = [results[p][f'val_{metric}'] * 100 for p in proportions]\n",
    "        test_metric = [results[p][f'test_{metric}'] * 100 for p in proportions]\n",
    "        \n",
    "        axes[i].plot(x_axis, train_metric, 'b-o', label='Training')\n",
    "        axes[i].plot(x_axis, val_metric, 'r-o', label='Validation')\n",
    "        axes[i].plot(x_axis, test_metric, 'g-o', label='Test')\n",
    "        axes[i].set_xlabel('Percentage of Soft Labels')\n",
    "        axes[i].set_ylabel(f'{title} (%)')\n",
    "        axes[i].set_title(f'{title} vs Proportion of Soft Labels')\n",
    "        axes[i].legend()\n",
    "        axes[i].grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('results/metrics.png')\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrices(results: Dict):\n",
    "    \"\"\"Plot confusion matrices for different proportions of soft labels.\"\"\"\n",
    "    proportions = list(results.keys())\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, prop in enumerate(proportions):\n",
    "        cm = results[prop]['test_confusion_matrix']  # Use test set confusion matrix\n",
    "        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        sns.heatmap(cm_normalized, annot=True, fmt='.2f', ax=axes[i], cmap='Blues')\n",
    "        axes[i].set_title(f'Normalized Confusion Matrix ({int(prop*100)}% Soft Labels)')\n",
    "        axes[i].set_xlabel('Predicted')\n",
    "        axes[i].set_ylabel('True')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('results/confusion_matrices.png')\n",
    "    plt.show()\n",
    "\n",
    "def plot_roc_pr_curves(results: Dict):\n",
    "    \"\"\"Plot ROC and Precision-Recall curves.\"\"\"\n",
    "    proportions = list(results.keys())\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "    for prop in proportions:\n",
    "        test_true = results[prop]['test_true_labels']\n",
    "        test_probs = results[prop]['test_pred_probabilities']\n",
    "        \n",
    "        # One-vs-Rest ROC curves\n",
    "        fpr = dict()\n",
    "        tpr = dict()\n",
    "        roc_auc = dict()\n",
    "        \n",
    "        # Convert to one-hot format\n",
    "        test_true_bin = np.eye(10)[test_true]\n",
    "        \n",
    "        for class_idx in range(10):\n",
    "            fpr[class_idx], tpr[class_idx], _ = roc_curve(\n",
    "                test_true_bin[:, class_idx], test_probs[:, class_idx])\n",
    "            roc_auc[class_idx] = auc(fpr[class_idx], tpr[class_idx])\n",
    "        \n",
    "        # Compute micro-average ROC curve and ROC area\n",
    "        fpr_micro, tpr_micro, _ = roc_curve(test_true_bin.ravel(), test_probs.ravel())\n",
    "        roc_auc_micro = auc(fpr_micro, tpr_micro)\n",
    "        \n",
    "        ax1.plot(fpr_micro, tpr_micro, \n",
    "                label=f'{int(prop*100)}% Soft Labels (AUC = {roc_auc_micro:.2f})')\n",
    "        \n",
    "        # PR curve\n",
    "        precision = dict()\n",
    "        recall = dict()\n",
    "        pr_auc = dict()\n",
    "        \n",
    "        for class_idx in range(10):\n",
    "            precision[class_idx], recall[class_idx], _ = precision_recall_curve(\n",
    "                test_true_bin[:, class_idx], test_probs[:, class_idx])\n",
    "            pr_auc[class_idx] = auc(recall[class_idx], precision[class_idx])\n",
    "        \n",
    "        # Compute micro-average PR curve\n",
    "        precision_micro, recall_micro, _ = precision_recall_curve(\n",
    "            test_true_bin.ravel(), test_probs.ravel())\n",
    "        pr_auc_micro = auc(recall_micro, precision_micro)\n",
    "        \n",
    "        ax2.plot(recall_micro, precision_micro,\n",
    "                label=f'{int(prop*100)}% Soft Labels (AUC = {pr_auc_micro:.2f})')\n",
    "\n",
    "    ax1.plot([0, 1], [0, 1], 'k--')\n",
    "    ax1.set_xlabel('False Positive Rate')\n",
    "    ax1.set_ylabel('True Positive Rate')\n",
    "    ax1.set_title('ROC Curves (Micro-averaged)')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "\n",
    "    ax2.set_xlabel('Recall')\n",
    "    ax2.set_ylabel('Precision')\n",
    "    ax2.set_title('Precision-Recall Curves (Micro-averaged)')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('results/roc_pr_curves.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "CIFAR-10 dataset loaded with 35000 augment, 7500 test, and 7500 validation samples\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "cifar10_datasets = load_cifar10_experiment()\n",
    "cifar10_hard_augment_dataset, _, cifar10_hard_test_dataset, cifar10_hard_val_dataset = cifar10_datasets\n",
    "\n",
    "# Create data loaders\n",
    "cifar10_hard_test_loader = DataLoader(cifar10_hard_test_dataset, batch_size=128, shuffle=False)\n",
    "cifar10_hard_val_loader = DataLoader(cifar10_hard_val_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "print(\n",
    "    f\"CIFAR-10 dataset loaded with \"\n",
    "    f\"{len(cifar10_hard_augment_dataset)} augment, \"\n",
    "    f\"{len(cifar10_hard_test_dataset)} test, and \"\n",
    "    f\"{len(cifar10_hard_val_dataset)} validation samples\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageHardToSoftLabelModel(\n",
       "  (image_encoder): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Sequential(\n",
       "      (0): ResidualBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): ResidualBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): ResidualBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (6): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  )\n",
       "  (label_encoder): Sequential(\n",
       "    (0): Linear(in_features=10, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.3, inplace=False)\n",
       "    (3): Linear(in_features=256, out_features=512, bias=True)\n",
       "  )\n",
       "  (attention): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Dropout(p=0.3, inplace=False)\n",
       "    (4): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): Dropout(p=0.3, inplace=False)\n",
       "    (8): Linear(in_features=256, out_features=10, bias=True)\n",
       "    (9): TemperatureSoftmax()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and prepare soft label model\n",
    "device = get_device()\n",
    "soft_label_model = ImageHardToSoftLabelModel().to(device)\n",
    "soft_label_model.load_state_dict(torch.load(\"models/soft_label_model.pt\", weights_only=True))\n",
    "soft_label_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running experiment with 0% soft labels\n",
      "Training on mps\n",
      "\n",
      "Training ResNet...\n",
      "Epoch [1/30] Train Loss: 0.9390, Validation Loss: 0.7972, Accuracy: 72.27%\n",
      "Saved model with improved validation accuracy: 72.27%\n",
      "Epoch [2/30] Train Loss: 0.6016, Validation Loss: 0.7051, Accuracy: 76.87%\n",
      "Saved model with improved validation accuracy: 76.87%\n",
      "Epoch [3/30] Train Loss: 0.4495, Validation Loss: 0.6291, Accuracy: 79.39%\n",
      "Saved model with improved validation accuracy: 79.39%\n",
      "Epoch [4/30] Train Loss: 0.3373, Validation Loss: 1.1099, Accuracy: 69.12%\n",
      "Epoch [5/30] Train Loss: 0.2560, Validation Loss: 0.6963, Accuracy: 78.51%\n",
      "Epoch [6/30] Train Loss: 0.2068, Validation Loss: 0.8395, Accuracy: 75.88%\n",
      "Epoch [7/30] Train Loss: 0.1680, Validation Loss: 0.8001, Accuracy: 78.87%\n",
      "Epoch [8/30] Train Loss: 0.1191, Validation Loss: 0.7687, Accuracy: 79.35%\n",
      "Epoch [9/30] Train Loss: 0.0999, Validation Loss: 0.8124, Accuracy: 79.80%\n",
      "Saved model with improved validation accuracy: 79.80%\n",
      "Epoch [10/30] Train Loss: 0.0923, Validation Loss: 0.7836, Accuracy: 80.37%\n",
      "Saved model with improved validation accuracy: 80.37%\n",
      "Epoch [11/30] Train Loss: 0.0802, Validation Loss: 0.8597, Accuracy: 79.23%\n",
      "Epoch [12/30] Train Loss: 0.0683, Validation Loss: 0.9254, Accuracy: 78.53%\n",
      "Epoch [13/30] Train Loss: 0.0706, Validation Loss: 0.8632, Accuracy: 79.80%\n",
      "Epoch [14/30] Train Loss: 0.0556, Validation Loss: 1.1730, Accuracy: 75.04%\n",
      "Epoch [15/30] Train Loss: 0.0783, Validation Loss: 0.8533, Accuracy: 78.33%\n",
      "Epoch [16/30] Train Loss: 0.0790, Validation Loss: 0.9055, Accuracy: 79.65%\n",
      "Epoch [17/30] Train Loss: 0.0587, Validation Loss: 1.1400, Accuracy: 75.49%\n",
      "Epoch [18/30] Train Loss: 0.0511, Validation Loss: 1.0102, Accuracy: 79.11%\n"
     ]
    }
   ],
   "source": [
    "# Run experiments\n",
    "results = run_proportion_experiment(\n",
    "    full_dataset=cifar10_hard_augment_dataset,\n",
    "    soft_label_model=soft_label_model,\n",
    "    val_loader=cifar10_hard_val_loader,\n",
    "    test_loader=cifar10_hard_test_loader,\n",
    "    soft_proportions=[0.0, 0.25, 0.5, 0.75, 1.0],\n",
    "    num_epochs=30,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "df = pd.DataFrame(results).T * 100\n",
    "df.index = [f\"{idx:.0f}%\" for idx in df.index * 100]\n",
    "print(\"\\nResults by Soft Label Percentage:\")\n",
    "print(\"================================\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_basic_metrics(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrices(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_pr_curves(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs229_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
