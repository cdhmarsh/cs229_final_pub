{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This file contains the code to run experiments with artificial soft labels.\n",
    "\n",
    "The experiment is:\n",
    "    * Train a soft label predictor model on CIFAR-10H\n",
    "    * Generate artificial soft labels for CIFAR-10\n",
    "    * Train a model on CIFAR-10 with the artificial soft labels + CIFAR-10H\n",
    "    * Evaluate the model on CIFAR-10\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from torchvision import datasets, transforms, models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from typing import Tuple\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-10 dataset and return train, validation, and test DataLoaders\n",
    "def load_cifar10() -> Tuple[Dataset, Dataset, Dataset]:\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.ConvertImageDtype(torch.float32),\n",
    "        ]\n",
    "    )\n",
    "    full_dataset = datasets.CIFAR10(root=\"../data/cifar-10\", train=True, download=True, transform=transform)\n",
    "    # we use the test dataset for training, similar to the CIFAR-10H experiment\n",
    "    train_dataset = datasets.CIFAR10(root=\"../data/cifar-10\", train=False, download=True, transform=transform)\n",
    "\n",
    "    # This dataset will be used for testing and validation.\n",
    "    #   30% of the data will be used for validation, and 70% for testing.\n",
    "    test_size = int(0.7 * len(full_dataset))\n",
    "    val_size = len(full_dataset) - test_size\n",
    "    test_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        full_dataset, [test_size, val_size], generator=torch.Generator().manual_seed(229)\n",
    "    )\n",
    "\n",
    "    return train_dataset, test_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "CIFAR-10 dataset loaded with 10000 training, 35000 test, and 15000 validation samples\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    cifar10_train_dataset,\n",
    "    cifar10_test_dataset,\n",
    "    cifar10_val_dataset,\n",
    ") = load_cifar10()\n",
    "\n",
    "cifar10_train_loader = DataLoader(cifar10_train_dataset, batch_size=128, shuffle=True)\n",
    "cifar10_test_loader = DataLoader(cifar10_test_dataset, batch_size=128, shuffle=False)\n",
    "cifar10_val_loader = DataLoader(cifar10_val_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "print(\n",
    "    f\"CIFAR-10 dataset loaded with {len(cifar10_train_dataset)} training, {len(cifar10_test_dataset)} test, and {len(cifar10_val_dataset)} validation samples\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-10 dataset and return augment, train, validation, and test DataLoaders\n",
    "def load_cifar10_experiment():\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.ConvertImageDtype(torch.float32),\n",
    "        ]\n",
    "    )\n",
    "    full_dataset = datasets.CIFAR10(root=\"../data/cifar-10\", train=True, download=True, transform=transform)\n",
    "    # we use the test dataset for training, similar to the CIFAR-10H experiment\n",
    "    train_dataset = datasets.CIFAR10(root=\"../data/cifar-10\", train=False, download=True, transform=transform)\n",
    "\n",
    "    # This dataset will be used for augmenting, testing, and validation.\n",
    "    augment_size = int(0.7 * len(full_dataset))\n",
    "    val_size = (len(full_dataset) - augment_size) // 2\n",
    "    test_size = len(full_dataset) - augment_size - val_size\n",
    "    augment_dataset, test_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        full_dataset, [augment_size, test_size, val_size], generator=torch.Generator().manual_seed(229)\n",
    "    )\n",
    "\n",
    "    return augment_dataset, train_dataset, test_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "CIFAR-10 dataset loaded with 35000 augment, 10000 training, 7500 test, and 7500 validation samples\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    cifar10_hard_augment_dataset,\n",
    "    cifar10_hard_train_dataset,\n",
    "    cifar10_hard_test_dataset,\n",
    "    cifar10_hard_val_dataset,\n",
    ") = load_cifar10_experiment()\n",
    "\n",
    "combined_hard_dataset = ConcatDataset([cifar10_hard_augment_dataset, cifar10_hard_train_dataset])\n",
    "cifar10_hard_augment_loader = DataLoader(cifar10_hard_augment_dataset, batch_size=128, shuffle=False)\n",
    "cifar10_hard_combined_loader = DataLoader(combined_hard_dataset, batch_size=128, shuffle=True)\n",
    "cifar10_hard_test_loader = DataLoader(cifar10_hard_test_dataset, batch_size=128, shuffle=False)\n",
    "cifar10_hard_val_loader = DataLoader(cifar10_hard_val_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "print(\n",
    "    f\"CIFAR-10 dataset loaded with {len(cifar10_hard_augment_dataset)} augment, {len(cifar10_hard_train_dataset)} training, {len(cifar10_hard_test_dataset)} test, and {len(cifar10_hard_val_dataset)} validation samples\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "Training is done on the CIFAR-10H dataset. Evaluation is done on the CIFAR-10 train set, which we use as a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    optimizer: optim.Optimizer,\n",
    "    num_epochs: int,\n",
    ") -> nn.Module:\n",
    "    device = torch.device(\n",
    "        \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    )\n",
    "    print(f\"Using device: {device}\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(images)\n",
    "\n",
    "                if len(labels.shape) > 1:  # For soft labels\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    _, labels = torch.max(labels, 1)\n",
    "                else:  # For hard labels\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                val_loss += criterion(outputs, labels).item()\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        print(\n",
    "            f\"Epoch [{epoch+1}/{num_epochs}] Train Loss: {running_loss/len(train_loader):.4f}, Validation Loss: {val_loss:.4f}, Accuracy: {accuracy:.2f}%\"\n",
    "        )\n",
    "\n",
    "        # Save model if validation accuracy improves\n",
    "        if accuracy > best_val_acc:\n",
    "            best_val_acc = accuracy\n",
    "            torch.save(model.state_dict(), f\"models/{model.__class__.__name__}_cifar10h.pth\")\n",
    "            print(f\"Saved model with improved validation accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn_model(model, cifar10h_loader, cifar10_val_loader, num_epochs=20):\n",
    "    print(f\"\\nTraining {model.__class__.__name__} on CIFAR-10H...\")\n",
    "\n",
    "    # Adjust the final layer for CIFAR-10\n",
    "    if isinstance(model, models.ResNet):\n",
    "        num_ftrs = model.fc.in_features\n",
    "        model.fc = nn.Linear(num_ftrs, 10)\n",
    "    elif isinstance(model, models.VGG):\n",
    "        num_ftrs = model.classifier[-1].in_features\n",
    "        model.classifier[-1] = nn.Linear(num_ftrs, 10)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    model = train_model(\n",
    "        model=model,\n",
    "        train_loader=cifar10h_loader,\n",
    "        val_loader=cifar10_val_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        num_epochs=num_epochs,\n",
    "    )\n",
    "\n",
    "\n",
    "def evaluate_nn_model(model, cifar10_test_loader):\n",
    "    model.load_state_dict(\n",
    "        torch.load(f\"models/{model.__class__.__name__}_cifar10h.pth\", weights_only=True)\n",
    "    )\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    device = next(model.parameters()).device\n",
    "    with torch.no_grad():\n",
    "        for images, labels in cifar10_test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"{model.__class__.__name__} Accuracy on CIFAR-10 test set: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageHardToSoftLabelModel(\n",
       "  (image_encoder): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Sequential(\n",
       "      (0): ResidualBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): ResidualBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): ResidualBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (6): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  )\n",
       "  (label_encoder): Sequential(\n",
       "    (0): Linear(in_features=10, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.3, inplace=False)\n",
       "    (3): Linear(in_features=256, out_features=512, bias=True)\n",
       "  )\n",
       "  (attention): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Dropout(p=0.3, inplace=False)\n",
       "    (4): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): Dropout(p=0.3, inplace=False)\n",
       "    (8): Linear(in_features=256, out_features=10, bias=True)\n",
       "    (9): TemperatureSoftmax()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from generate_soft_labels import create_soft_label_dataloader, create_soft_label_dataset\n",
    "from soft_label_predictor import ImageHardToSoftLabelModel\n",
    "\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "# Load the trained model\n",
    "model = ImageHardToSoftLabelModel().to(device)\n",
    "model.load_state_dict(torch.load(\"models/soft_label_model.pt\", weights_only=True))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 10000 samples and validating on 15000 samples\n",
      "\n",
      "Training ResNet on CIFAR-10H...\n",
      "Using device: mps\n",
      "Epoch [1/20] Train Loss: 1.2972, Validation Loss: 1.0189, Accuracy: 64.85%\n",
      "Saved model with improved validation accuracy: 64.85%\n",
      "Epoch [2/20] Train Loss: 0.8447, Validation Loss: 1.0859, Accuracy: 64.45%\n",
      "Epoch [3/20] Train Loss: 0.7351, Validation Loss: 1.0805, Accuracy: 63.88%\n",
      "Epoch [4/20] Train Loss: 0.5767, Validation Loss: 0.8231, Accuracy: 72.91%\n",
      "Saved model with improved validation accuracy: 72.91%\n",
      "Epoch [5/20] Train Loss: 0.4660, Validation Loss: 1.0465, Accuracy: 67.19%\n",
      "Epoch [6/20] Train Loss: 0.4312, Validation Loss: 0.9718, Accuracy: 69.98%\n",
      "Epoch [7/20] Train Loss: 0.3207, Validation Loss: 0.8505, Accuracy: 73.70%\n",
      "Saved model with improved validation accuracy: 73.70%\n",
      "Epoch [8/20] Train Loss: 0.2987, Validation Loss: 0.9657, Accuracy: 70.81%\n",
      "Epoch [9/20] Train Loss: 0.2967, Validation Loss: 0.9251, Accuracy: 71.61%\n",
      "Epoch [10/20] Train Loss: 0.2800, Validation Loss: 0.8717, Accuracy: 73.45%\n",
      "Epoch [11/20] Train Loss: 0.2399, Validation Loss: 0.9844, Accuracy: 71.18%\n",
      "Epoch [12/20] Train Loss: 0.3503, Validation Loss: 0.9291, Accuracy: 72.02%\n",
      "Epoch [13/20] Train Loss: 0.2997, Validation Loss: 0.9030, Accuracy: 72.03%\n",
      "Epoch [14/20] Train Loss: 0.3189, Validation Loss: 1.0148, Accuracy: 70.14%\n",
      "Epoch [15/20] Train Loss: 0.2819, Validation Loss: 0.8647, Accuracy: 73.71%\n",
      "Saved model with improved validation accuracy: 73.71%\n",
      "Epoch [16/20] Train Loss: 0.2347, Validation Loss: 0.8182, Accuracy: 75.08%\n",
      "Saved model with improved validation accuracy: 75.08%\n",
      "Epoch [17/20] Train Loss: 0.2304, Validation Loss: 0.8128, Accuracy: 75.19%\n",
      "Saved model with improved validation accuracy: 75.19%\n",
      "Epoch [18/20] Train Loss: 0.2674, Validation Loss: 0.9667, Accuracy: 71.74%\n",
      "Epoch [19/20] Train Loss: 0.2321, Validation Loss: 0.8046, Accuracy: 75.39%\n",
      "Saved model with improved validation accuracy: 75.39%\n",
      "Epoch [20/20] Train Loss: 0.1992, Validation Loss: 0.7555, Accuracy: 76.80%\n",
      "Saved model with improved validation accuracy: 76.80%\n",
      "ResNet Accuracy on CIFAR-10 test set: 77.21%\n"
     ]
    }
   ],
   "source": [
    "# Sanity check - evaluate model trained on CIFAR-10H augmenting CIFAR-10 equivalent set. Since this is\n",
    "# basically reproducing CIFAR-10H, we expect the model to perform similarly (little worse) than just\n",
    "# training with CIFAR-10H directly.\n",
    "\n",
    "# Convert to DataLoader with predicted soft labels\n",
    "soft_label_dataloader = create_soft_label_dataloader(\n",
    "    model, cifar10_train_loader, batch_size=128, device=device\n",
    ")\n",
    "print(\n",
    "    f\"Training on {len(soft_label_dataloader.dataset)} samples and validating on {len(cifar10_val_loader.dataset)} samples\"\n",
    ")\n",
    "\n",
    "resnet_model = models.resnet34(weights=models.ResNet34_Weights.DEFAULT)\n",
    "train_nn_model(resnet_model, soft_label_dataloader, cifar10_val_loader)\n",
    "evaluate_nn_model(resnet_model, cifar10_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new class to handle both hard and soft labels consistently\n",
    "class CIFAR10LabelDataset(Dataset):\n",
    "    def __init__(self, dataset, soft_labels=None):\n",
    "        self.dataset = dataset\n",
    "        self.soft_labels = soft_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.dataset[idx]\n",
    "        if self.soft_labels is None:\n",
    "            # Convert hard labels to one-hot\n",
    "            label = F.one_hot(torch.tensor(label), num_classes=10).float()\n",
    "        else:\n",
    "            label = torch.tensor(self.soft_labels[idx])\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 45000 samples and validating on 7500 samples\n",
      "\n",
      "Training ResNet on CIFAR-10H...\n",
      "Using device: mps\n",
      "Epoch [1/50] Train Loss: 0.9432, Validation Loss: 0.7169, Accuracy: 76.21%\n",
      "Saved model with improved validation accuracy: 76.21%\n",
      "Epoch [2/50] Train Loss: 0.6404, Validation Loss: 0.7421, Accuracy: 75.72%\n",
      "Epoch [3/50] Train Loss: 0.5254, Validation Loss: 0.6887, Accuracy: 76.56%\n",
      "Saved model with improved validation accuracy: 76.56%\n",
      "Epoch [4/50] Train Loss: 0.4340, Validation Loss: 0.6201, Accuracy: 80.19%\n",
      "Saved model with improved validation accuracy: 80.19%\n",
      "Epoch [5/50] Train Loss: 0.3499, Validation Loss: 0.6506, Accuracy: 79.09%\n",
      "Epoch [6/50] Train Loss: 0.2923, Validation Loss: 0.6045, Accuracy: 80.93%\n",
      "Saved model with improved validation accuracy: 80.93%\n",
      "Epoch [7/50] Train Loss: 0.2665, Validation Loss: 0.6213, Accuracy: 81.21%\n",
      "Saved model with improved validation accuracy: 81.21%\n",
      "Epoch [8/50] Train Loss: 0.2394, Validation Loss: 0.6381, Accuracy: 81.52%\n",
      "Saved model with improved validation accuracy: 81.52%\n",
      "Epoch [9/50] Train Loss: 0.2252, Validation Loss: 0.7282, Accuracy: 78.47%\n",
      "Epoch [10/50] Train Loss: 0.2082, Validation Loss: 0.6150, Accuracy: 81.99%\n",
      "Saved model with improved validation accuracy: 81.99%\n",
      "Epoch [11/50] Train Loss: 0.2019, Validation Loss: 0.6397, Accuracy: 81.75%\n",
      "Epoch [12/50] Train Loss: 0.2275, Validation Loss: 0.6280, Accuracy: 81.44%\n",
      "Epoch [13/50] Train Loss: 0.1931, Validation Loss: 0.6772, Accuracy: 80.96%\n",
      "Epoch [14/50] Train Loss: 0.1835, Validation Loss: 0.6809, Accuracy: 80.85%\n",
      "Epoch [15/50] Train Loss: 0.1822, Validation Loss: 0.6886, Accuracy: 80.52%\n"
     ]
    }
   ],
   "source": [
    "# Experiment - evaluate model trained on CIFAR-10H augmenting CIFAR-10 larger train set.\n",
    "\n",
    "cifar10h_probs_path = \"../data/cifar-10h/cifar10h-probs.npy\"\n",
    "cifar10h_probs = np.load(cifar10h_probs_path).astype(np.float32)\n",
    "\n",
    "cifar10_soft_label_dataset = CIFAR10LabelDataset(cifar10_hard_train_dataset, cifar10h_probs)\n",
    "augmented_dataset = create_soft_label_dataset(model, cifar10_hard_augment_loader, device)\n",
    "\n",
    "# This dataset is fully soft labels\n",
    "combined_train_dataset = ConcatDataset([augmented_dataset, cifar10_soft_label_dataset])\n",
    "combined_train_loader = DataLoader(combined_train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "print(\n",
    "    f\"Training on {len(combined_train_loader.dataset)} samples and validating on {len(cifar10_hard_val_loader.dataset)} samples\"\n",
    ")\n",
    "\n",
    "resnet_model = models.resnet34(weights=models.ResNet34_Weights.DEFAULT)\n",
    "train_nn_model(resnet_model, combined_train_loader, cifar10_hard_val_loader, num_epochs=30)\n",
    "evaluate_nn_model(resnet_model, cifar10_hard_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline 1 - evaluate model trained on cifar10 hard labels + cifar10h hard labels\n",
    "print(\n",
    "    f\"Training on {len(cifar10_hard_combined_loader.dataset)} samples and validating on {len(cifar10_hard_val_loader.dataset)} samples\"\n",
    ")\n",
    "\n",
    "resnet_model = models.resnet34(weights=models.ResNet34_Weights.DEFAULT)\n",
    "train_nn_model(resnet_model, cifar10_hard_combined_loader, cifar10_hard_val_loader, num_epochs=30)\n",
    "evaluate_nn_model(resnet_model, cifar10_hard_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline 2 - evaluate model trained on cifar10 hard labels + cifar10h soft labels\n",
    "cifar10h_probs_path = \"../data/cifar-10h/cifar10h-probs.npy\"\n",
    "cifar10h_probs = np.load(cifar10h_probs_path).astype(np.float32)\n",
    "\n",
    "# This dataset is partially soft and partially hard labels\n",
    "cifar10_soft_label_dataset = CIFAR10LabelDataset(cifar10_hard_train_dataset, cifar10h_probs)\n",
    "cifar10_hard_label_dataset = CIFAR10LabelDataset(cifar10_hard_augment_dataset)\n",
    "combined_train_dataset = ConcatDataset([cifar10_hard_label_dataset, cifar10_soft_label_dataset])\n",
    "\n",
    "cifar10_baseline_train_loader = DataLoader(combined_train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "print(\n",
    "    f\"Training on {len(cifar10_baseline_train_loader.dataset)} samples and validating on {len(cifar10_hard_val_loader.dataset)} samples\"\n",
    ")\n",
    "print(\n",
    "    f\"    Train set has {len(cifar10_hard_augment_dataset)} hard labels and {len(cifar10_soft_label_dataset)} soft labels\"\n",
    ")\n",
    "\n",
    "resnet_model = models.resnet34(weights=models.ResNet34_Weights.DEFAULT)\n",
    "train_nn_model(resnet_model, cifar10_baseline_train_loader, cifar10_hard_val_loader, num_epochs=30)\n",
    "evaluate_nn_model(resnet_model, cifar10_hard_test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs229_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
