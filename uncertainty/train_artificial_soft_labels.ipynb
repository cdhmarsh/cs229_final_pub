{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This file contains the code to run the baseline experiments.\n",
    "\n",
    "More specifically, we are investigating the performance of basic models on the CIFAR-10 and CIFAR-10H datasets. The tasks for these datasets are multi-class classification.\n",
    "\n",
    "The basic models include:\n",
    "    * ResNet-50\n",
    "    * VGG-16\n",
    "    * Logistic Regression\n",
    "    * Random Forest\n",
    "    * XGBoost\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-10 dataset and return train, validation, and test DataLoaders\n",
    "def load_cifar10() -> Tuple[Dataset, Dataset, Dataset]:\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.ConvertImageDtype(torch.float32),\n",
    "        ]\n",
    "    )\n",
    "    full_dataset = datasets.CIFAR10(root=\"../data/cifar-10\", train=True, download=True, transform=transform)\n",
    "    # we use the test dataset for training, similar to the CIFAR-10H experiment\n",
    "    train_dataset = datasets.CIFAR10(root=\"../data/cifar-10\", train=False, download=True, transform=transform)\n",
    "\n",
    "    # This dataset will be used for testing and validation.\n",
    "    #   30% of the data will be used for validation, and 70% for testing.\n",
    "    test_size = int(0.7 * len(full_dataset))\n",
    "    val_size = len(full_dataset) - test_size\n",
    "    test_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        full_dataset, [test_size, val_size], generator=torch.Generator().manual_seed(229)\n",
    "    )\n",
    "\n",
    "    return train_dataset, test_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "CIFAR-10 dataset loaded with 10000 training, 35000 test, and 15000 validation samples\n"
     ]
    }
   ],
   "source": [
    "cifar10_train_dataset, cifar10_test_dataset, cifar10_val_dataset = load_cifar10()  # Changed variable name to reflect split\n",
    "cifar10_train_loader = DataLoader(cifar10_train_dataset, batch_size=128, shuffle=True)\n",
    "cifar10_test_loader = DataLoader(cifar10_test_dataset, batch_size=128, shuffle=False)\n",
    "cifar10_val_loader = DataLoader(cifar10_val_dataset, batch_size=128, shuffle=False)\n",
    "print(\n",
    "    f\"CIFAR-10 dataset loaded with {len(cifar10_train_dataset)} training, {len(cifar10_test_dataset)} test, and {len(cifar10_val_dataset)} validation samples\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "Training is done on the CIFAR-10H dataset. Evaluation is done on the CIFAR-10 train set, which we use as a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    optimizer: optim.Optimizer,\n",
    "    num_epochs: int,\n",
    ") -> nn.Module:\n",
    "    device = torch.device(\n",
    "        \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    )\n",
    "    print(f\"Using device: {device}\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(images)\n",
    "\n",
    "                if len(labels.shape) > 1:  # For soft labels\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    _, labels = torch.max(labels, 1)\n",
    "                else:  # For hard labels\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                val_loss += criterion(outputs, labels).item()\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        print(\n",
    "            f\"Epoch [{epoch+1}/{num_epochs}] Train Loss: {running_loss/len(train_loader):.4f}, Validation Loss: {val_loss:.4f}, Accuracy: {accuracy:.2f}%\"\n",
    "        )\n",
    "\n",
    "        # Save model if validation accuracy improves\n",
    "        if accuracy > best_val_acc:\n",
    "            best_val_acc = accuracy\n",
    "            torch.save(model.state_dict(), f\"models/{model.__class__.__name__}_cifar10h.pth\")\n",
    "            print(f\"Saved model with improved validation accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn_model(\n",
    "    model, cifar10h_loader: DataLoader, cifar10_val_loader: DataLoader, num_epochs: int = 20, lr: float = 0.001\n",
    ") -> list:\n",
    "    print(f\"\\nTraining {model.__class__.__name__} on CIFAR-10H...\")\n",
    "\n",
    "    # Adjust the final layer for CIFAR-10\n",
    "    if isinstance(model, models.ResNet):\n",
    "        num_ftrs = model.fc.in_features\n",
    "        model.fc = nn.Linear(num_ftrs, 10)\n",
    "    elif isinstance(model, models.VGG):\n",
    "        num_ftrs = model.classifier[-1].in_features\n",
    "        model.classifier[-1] = nn.Linear(num_ftrs, 10)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    model = train_model(\n",
    "        model=model,\n",
    "        train_loader=cifar10h_loader,\n",
    "        val_loader=cifar10_val_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        num_epochs=num_epochs,\n",
    "    )\n",
    "\n",
    "def evaluate_nn_model(model, cifar10_test_loader):\n",
    "    model.load_state_dict(\n",
    "        torch.load(f\"models/{model.__class__.__name__}_cifar10h.pth\", weights_only=True)\n",
    "    )\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    device = next(model.parameters()).device\n",
    "    with torch.no_grad():\n",
    "        for images, labels in cifar10_test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"{model.__class__.__name__} Accuracy on CIFAR-10 test set: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate_soft_labels import create_soft_label_dataloader\n",
    "from soft_label_predictor import ImageHardLabelToSoftLabelModel\n",
    "\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    )\n",
    "\n",
    "# Load the trained model\n",
    "model = ImageHardLabelToSoftLabelModel().to(device)\n",
    "model.load_state_dict(torch.load(\"models/best_model.pt\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training ResNet on CIFAR-10H...\n",
      "Using device: mps\n",
      "Epoch [1/20] Train Loss: 1.2828, Validation Loss: 1.2012, Accuracy: 60.23%\n",
      "Saved model with improved validation accuracy: 60.23%\n",
      "Epoch [2/20] Train Loss: 0.8598, Validation Loss: 1.0033, Accuracy: 66.71%\n",
      "Saved model with improved validation accuracy: 66.71%\n",
      "Epoch [3/20] Train Loss: 0.6887, Validation Loss: 0.9057, Accuracy: 69.31%\n",
      "Saved model with improved validation accuracy: 69.31%\n",
      "Epoch [4/20] Train Loss: 0.5440, Validation Loss: 0.8316, Accuracy: 72.31%\n",
      "Saved model with improved validation accuracy: 72.31%\n",
      "Epoch [5/20] Train Loss: 0.4893, Validation Loss: 0.8644, Accuracy: 71.87%\n",
      "Epoch [6/20] Train Loss: 0.4712, Validation Loss: 0.8267, Accuracy: 73.95%\n",
      "Saved model with improved validation accuracy: 73.95%\n",
      "Epoch [7/20] Train Loss: 0.3672, Validation Loss: 0.8603, Accuracy: 72.97%\n",
      "Epoch [8/20] Train Loss: 0.3285, Validation Loss: 0.8966, Accuracy: 71.89%\n",
      "Epoch [9/20] Train Loss: 0.2946, Validation Loss: 0.8479, Accuracy: 73.67%\n",
      "Epoch [10/20] Train Loss: 0.2882, Validation Loss: 0.8513, Accuracy: 73.85%\n",
      "Epoch [11/20] Train Loss: 0.3690, Validation Loss: 0.9322, Accuracy: 71.73%\n",
      "Epoch [12/20] Train Loss: 0.3897, Validation Loss: 0.9309, Accuracy: 70.93%\n",
      "Epoch [13/20] Train Loss: 0.3186, Validation Loss: 0.8871, Accuracy: 72.62%\n",
      "Epoch [14/20] Train Loss: 0.2812, Validation Loss: 0.7995, Accuracy: 75.09%\n",
      "Saved model with improved validation accuracy: 75.09%\n",
      "Epoch [15/20] Train Loss: 0.2589, Validation Loss: 0.7553, Accuracy: 76.48%\n",
      "Saved model with improved validation accuracy: 76.48%\n",
      "Epoch [16/20] Train Loss: 0.2497, Validation Loss: 0.7722, Accuracy: 75.95%\n",
      "Epoch [17/20] Train Loss: 0.2662, Validation Loss: 0.8229, Accuracy: 74.57%\n",
      "Epoch [18/20] Train Loss: 0.2667, Validation Loss: 0.8184, Accuracy: 74.30%\n",
      "Epoch [19/20] Train Loss: 0.2732, Validation Loss: 0.8260, Accuracy: 74.93%\n",
      "Epoch [20/20] Train Loss: 0.2785, Validation Loss: 0.8585, Accuracy: 73.59%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexgu/opt/anaconda3/envs/fruittest/lib/python3.11/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet Accuracy on CIFAR-10 test set: 76.19%\n"
     ]
    }
   ],
   "source": [
    "# Sanity check - evaluate model trained on CIFAR-10H augmenting CIFAR-10 equivalent set. Since this is basically reproducing CIFAR-10H, we expect\n",
    "# the model to perform similarly (little worse) than just training with CIFAR-10H directly.\n",
    "\n",
    "# Convert to DataLoader with predicted soft labels\n",
    "soft_label_dataloader = create_soft_label_dataloader(model, cifar10_train_loader, batch_size=128, device=device)\n",
    "\n",
    "resnet_model = models.resnet34(weights=models.ResNet34_Weights.DEFAULT)\n",
    "train_nn_model(resnet_model, soft_label_dataloader, cifar10_val_loader, lr=0.01)\n",
    "evaluate_nn_model(resnet_model, cifar10_test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs229_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
