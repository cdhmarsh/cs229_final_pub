{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This file contains the code to run experiments with artificial soft labels.\n",
    "\n",
    "The experiment is:\n",
    "    * Train a soft label predictor model on CIFAR-10H\n",
    "    * Generate artificial soft labels for CIFAR-10\n",
    "    * Train a model on CIFAR-10 with the artificial soft labels + CIFAR-10H\n",
    "    * Evaluate the model on CIFAR-10\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from torchvision import datasets, transforms, models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from typing import Tuple, Dict, List\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "from generate_soft_labels import create_soft_label_dataset\n",
    "from soft_label_predictor import ImageHardToSoftLabelModel\n",
    "import os\n",
    "import json\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "CIFAR10_CLASSES = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "                   'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_curves(metrics: Dict[str, List[float]], title: str = \"Training Curves\"):\n",
    "    \"\"\"Plot training and validation metrics over epochs.\"\"\"\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    # Plot losses\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(metrics['train_loss'], label='Train Loss')\n",
    "    plt.plot(metrics['val_loss'], label='Val Loss')\n",
    "    plt.title('Loss Curves')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot accuracies\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(metrics['train_acc'], label='Train Acc')\n",
    "    plt.plot(metrics['val_acc'], label='Val Acc')\n",
    "    plt.title('Accuracy Curves')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes):\n",
    "    \"\"\"Plot confusion matrix.\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=classes, yticklabels=classes)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_metrics(metrics: Dict, model_name: str):\n",
    "    \"\"\"Save metrics dictionary to JSON file.\"\"\"\n",
    "    metrics_path = f\"metrics/{model_name}_metrics.json\"\n",
    "    os.makedirs(\"metrics\", exist_ok=True)\n",
    "    \n",
    "    # Convert numpy arrays and tensors to lists for JSON serialization\n",
    "    serializable_metrics = {}\n",
    "    for key, value in metrics.items():\n",
    "        if isinstance(value, (np.ndarray, torch.Tensor)):\n",
    "            serializable_metrics[key] = value.tolist()\n",
    "        elif isinstance(value, list):\n",
    "            serializable_metrics[key] = [v.item() if isinstance(v, (np.ndarray, torch.Tensor)) else v for v in value]\n",
    "        else:\n",
    "            serializable_metrics[key] = value\n",
    "    \n",
    "    with open(metrics_path, 'w') as f:\n",
    "        json.dump(serializable_metrics, f)\n",
    "\n",
    "def plot_comparative_curves(all_metrics: Dict[str, Dict], title: str = \"Comparative Training Curves\"):\n",
    "    \"\"\"Plot training curves for multiple models on the same graph.\"\"\"\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Plot losses\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for model_name, metrics in all_metrics.items():\n",
    "        plt.plot(metrics['train_loss'], label=f'{model_name} Train')\n",
    "        plt.plot(metrics['val_loss'], label=f'{model_name} Val', linestyle='--')\n",
    "    plt.title('Loss Curves')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot accuracies\n",
    "    plt.subplot(1, 2, 2)\n",
    "    for model_name, metrics in all_metrics.items():\n",
    "        plt.plot(metrics['train_acc'], label=f'{model_name} Train')\n",
    "        plt.plot(metrics['val_acc'], label=f'{model_name} Val', linestyle='--')\n",
    "    plt.title('Accuracy Curves')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('comparative_curves.png')\n",
    "    plt.show()\n",
    "\n",
    "def compute_additional_metrics(all_preds, all_labels, all_probs):\n",
    "    \"\"\"Compute additional metrics including AUPRC.\"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    # Compute precision, recall, and F1 for each class\n",
    "    for i in range(10):\n",
    "        binary_labels = (np.array(all_labels) == i).astype(int)\n",
    "        binary_probs = np.array(all_probs)[:, i]\n",
    "        \n",
    "        precision, recall, _ = precision_recall_curve(binary_labels, binary_probs)\n",
    "        auprc = average_precision_score(binary_labels, binary_probs)\n",
    "        \n",
    "        metrics[f'class_{CIFAR10_CLASSES[i]}_auprc'] = auprc\n",
    "    \n",
    "    # Compute macro-averaged metrics\n",
    "    metrics['macro_auprc'] = np.mean([metrics[f'class_{c}_auprc'] for c in CIFAR10_CLASSES])\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def evaluate_metrics(model, test_loader, device, classes=CIFAR10_CLASSES):\n",
    "    \"\"\"Evaluate model performance with detailed metrics.\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            if len(labels.shape) > 1:  # For soft labels\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                _, true_labels = torch.max(labels, 1)\n",
    "            else:  # For hard labels\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                true_labels = labels\n",
    "                \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == true_labels).sum().item()\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(true_labels.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "    # Calculate basic metrics\n",
    "    accuracy = 100 * correct / total\n",
    "    avg_loss = test_loss / len(test_loader)\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    additional_metrics = compute_additional_metrics(all_preds, all_labels, all_probs)\n",
    "    \n",
    "    report = classification_report(all_labels, all_preds, \n",
    "                                                    target_names=classes if classes else None,\n",
    "                                                    output_dict=True)\n",
    "    \n",
    "    # Print summary of metrics\n",
    "    print(f\"\\nTest Loss: {avg_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\") \n",
    "    print(f\"Macro AUPRC: {additional_metrics['macro_auprc']:.4f}\")\n",
    "    print(report)\n",
    "\n",
    "    \n",
    "    # Combine all metrics\n",
    "    metrics = {\n",
    "        'test_loss': avg_loss,\n",
    "        'test_accuracy': accuracy,\n",
    "        **additional_metrics,\n",
    "        'classification_report': report\n",
    "    }\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    num_epochs: int,\n",
    "    device: str = None,\n",
    "    learning_rate: float = 0.001,\n",
    "    weight_decay: float = 0.01,\n",
    "    model_name: str = None,\n",
    ") -> Tuple[nn.Module, Dict]:\n",
    "    if device is None:\n",
    "        device = torch.device(\n",
    "            \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "        )\n",
    "    print(f\"Training on {device}\")\n",
    "    \n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    \n",
    "    metrics = {\n",
    "        'train_loss': [], 'train_acc': [],\n",
    "        'val_loss': [], 'val_acc': []\n",
    "    }\n",
    "    best_val_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            if len(labels.shape) > 1:  # For soft labels\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                _, true_labels = torch.max(labels, 1)\n",
    "            else:  # For hard labels\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                true_labels = labels\n",
    "                \n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == true_labels).sum().item()\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        train_acc = 100 * train_correct / train_total\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                if len(labels.shape) > 1:  # For soft labels\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    _, true_labels = torch.max(labels, 1)\n",
    "                else:  # For hard labels\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    true_labels = labels\n",
    "                    \n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == true_labels).sum().item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_acc = 100 * val_correct / val_total\n",
    "        \n",
    "        # Store metrics\n",
    "        metrics['train_loss'].append(avg_train_loss)\n",
    "        metrics['train_acc'].append(train_acc)\n",
    "        metrics['val_loss'].append(avg_val_loss)\n",
    "        metrics['val_acc'].append(val_acc)\n",
    "        \n",
    "        print(\n",
    "            f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "            f\"Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, \"\n",
    "            f\"Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%\"\n",
    "        )\n",
    "\n",
    "        # Save model if validation accuracy improves\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            if model_name:\n",
    "                torch.save(model.state_dict(), f\"models/{model_name}.pth\")\n",
    "                print(f\"Saved model with improved validation accuracy: {val_acc:.2f}%\")\n",
    "\n",
    "    # Plot training curves\n",
    "    plot_training_curves(metrics, title=f\"Training Curves - {model_name if model_name else 'Model'}\")\n",
    "    \n",
    "    return model, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10LabelDataset(Dataset):\n",
    "    def __init__(self, dataset, soft_labels=None):\n",
    "        self.dataset = dataset\n",
    "        self.soft_labels = soft_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.dataset[idx]\n",
    "        if self.soft_labels is None:\n",
    "            # Convert hard labels to one-hot\n",
    "            label = F.one_hot(torch.tensor(label), num_classes=10).float()\n",
    "        else:\n",
    "            label = torch.tensor(self.soft_labels[idx])\n",
    "        return image, label\n",
    "    \n",
    "# Load CIFAR-10 dataset and return train, validation, and test DataLoaders\n",
    "def load_cifar10_experiment():\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.ConvertImageDtype(torch.float32),\n",
    "    ])\n",
    "    \n",
    "    full_dataset = datasets.CIFAR10(root=\"../data/cifar-10\", train=True, download=True, transform=transform)\n",
    "    train_dataset = datasets.CIFAR10(root=\"../data/cifar-10\", train=False, download=True, transform=transform)\n",
    "\n",
    "    # Split full dataset into augment, test, and validation sets\n",
    "    augment_size = int(0.7 * len(full_dataset))\n",
    "    val_size = (len(full_dataset) - augment_size) // 2\n",
    "    test_size = len(full_dataset) - augment_size - val_size\n",
    "    \n",
    "    augment_dataset, test_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        full_dataset, [augment_size, test_size, val_size], \n",
    "        generator=torch.Generator().manual_seed(229)\n",
    "    )\n",
    "\n",
    "    return augment_dataset, train_dataset, test_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() \n",
    "    else \"mps\" if torch.backends.mps.is_available() \n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load datasets\n",
    "augment_dataset, train_dataset, test_dataset, val_dataset = load_cifar10_experiment()\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "augment_loader = DataLoader(augment_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Load CIFAR-10H soft labels\n",
    "cifar10h_probs = np.load(\"../data/cifar-10h/cifar10h-probs.npy\").astype(np.float32)\n",
    "\n",
    "print(f\"CIFAR-10 dataset loaded with {len(augment_dataset)} augment samples, {len(train_dataset)} training samples, {len(test_dataset)} testing samples, and {len(val_dataset)} validation samples\")\n",
    "print(f\"CIFAR-10H soft labels loaded with shape {cifar10h_probs.shape}\")\n",
    "\n",
    "all_metrics = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Running Main Experiment ===\")\n",
    "print(\"Training with artificial soft labels + CIFAR-10H\")\n",
    "\n",
    "# Load soft label predictor model\n",
    "soft_label_model = ImageHardToSoftLabelModel().to(device)\n",
    "soft_label_model.load_state_dict(torch.load(\"models/soft_label_model.pt\", weights_only=True))\n",
    "soft_label_model.eval()\n",
    "\n",
    "# Generate artificial soft labels for augment dataset\n",
    "augmented_soft_dataset = create_soft_label_dataset(soft_label_model, augment_loader, device)\n",
    "cifar10h_soft_dataset = CIFAR10LabelDataset(train_dataset, cifar10h_probs)\n",
    "\n",
    "# Combine datasets and create loader\n",
    "combined_soft_dataset = ConcatDataset([augmented_soft_dataset, cifar10h_soft_dataset])\n",
    "combined_soft_loader = DataLoader(combined_soft_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(f\"Combined soft dataset size: {len(combined_soft_dataset)} samples\")\n",
    "print(f\"- Augmented soft labels: {len(augmented_soft_dataset)} samples\")\n",
    "print(f\"- CIFAR-10H soft labels: {len(cifar10h_soft_dataset)} samples\")\n",
    "\n",
    "# Train model\n",
    "model = models.resnet34(weights=models.ResNet34_Weights.DEFAULT)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 10)\n",
    "\n",
    "model, metrics = train_model(\n",
    "    model=model,\n",
    "    train_loader=combined_soft_loader,\n",
    "    val_loader=val_loader,\n",
    "    num_epochs=30,\n",
    "    device=device,\n",
    "    model_name=\"resnet34_artificial_soft\"\n",
    ")\n",
    "\n",
    "# Evaluate and save metrics for main experiment\n",
    "model_metrics = evaluate_metrics(model, test_loader, device)\n",
    "all_metrics['artificial_soft'] = {\n",
    "    **metrics,  # Training metrics\n",
    "    **model_metrics  # Test metrics\n",
    "}\n",
    "save_metrics(all_metrics['artificial_soft'], 'artificial_soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Running Baseline 1 ===\")\n",
    "print(\"Training with CIFAR-10 hard labels + CIFAR-10H hard labels\")\n",
    "\n",
    "# Create combined hard label dataset\n",
    "combined_hard_dataset = ConcatDataset([augment_dataset, train_dataset])\n",
    "combined_hard_loader = DataLoader(combined_hard_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(f\"Combined hard dataset size: {len(combined_hard_dataset)} samples\")\n",
    "print(f\"- Augment hard labels: {len(augment_dataset)} samples\")\n",
    "print(f\"- CIFAR-10H hard labels: {len(train_dataset)} samples\")\n",
    "\n",
    "model = models.resnet34(weights=models.ResNet34_Weights.DEFAULT)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 10)\n",
    "\n",
    "model, metrics = train_model(\n",
    "    model=model,\n",
    "    train_loader=combined_hard_loader,\n",
    "    val_loader=val_loader,\n",
    "    num_epochs=30,\n",
    "    device=device,\n",
    "    model_name=\"resnet34_hard_baseline\"\n",
    ")\n",
    "\n",
    "# Evaluate and save metrics for baseline 1\n",
    "model_metrics = evaluate_metrics(model, test_loader, device)\n",
    "all_metrics['hard_baseline'] = {\n",
    "    **metrics,  # Training metrics\n",
    "    **model_metrics  # Test metrics\n",
    "}\n",
    "save_metrics(all_metrics['hard_baseline'], 'hard_baseline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Running Baseline 2 ===\")\n",
    "print(\"Training with CIFAR-10 hard labels + CIFAR-10H soft labels\")\n",
    "\n",
    "# Create mixed dataset (hard + soft labels)\n",
    "hard_label_dataset = CIFAR10LabelDataset(augment_dataset)\n",
    "soft_label_dataset = CIFAR10LabelDataset(train_dataset, cifar10h_probs)\n",
    "combined_mixed_dataset = ConcatDataset([hard_label_dataset, soft_label_dataset])\n",
    "combined_mixed_loader = DataLoader(combined_mixed_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(f\"Combined mixed dataset size: {len(combined_mixed_dataset)} samples\")\n",
    "print(f\"- Augment hard labels: {len(hard_label_dataset)} samples\")\n",
    "print(f\"- CIFAR-10H soft labels: {len(soft_label_dataset)} samples\")\n",
    "\n",
    "model = models.resnet34(weights=models.ResNet34_Weights.DEFAULT)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 10)\n",
    "\n",
    "model, metrics = train_model(\n",
    "    model=model,\n",
    "    train_loader=combined_mixed_loader,\n",
    "    val_loader=val_loader,\n",
    "    num_epochs=30,\n",
    "    device=device,\n",
    "    model_name=\"resnet34_mixed_baseline\"\n",
    ")\n",
    "\n",
    "# Evaluate and save metrics for baseline 2\n",
    "model_metrics = evaluate_metrics(model, test_loader, device)\n",
    "all_metrics['mixed_baseline'] = {\n",
    "    **metrics,  # Training metrics\n",
    "    **model_metrics  # Test metrics\n",
    "}\n",
    "save_metrics(all_metrics['mixed_baseline'], 'mixed_baseline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparative curves\n",
    "plot_comparative_curves(all_metrics)\n",
    "\n",
    "# Print comparative metrics\n",
    "print(\"\\n=== Comparative Results ===\")\n",
    "for model_name, metrics in all_metrics.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"Test Accuracy: {metrics['test_accuracy']:.2f}%\")\n",
    "    print(f\"Macro AUPRC: {metrics['macro_auprc']:.4f}\")\n",
    "    print(\"Per-class AUPRC:\")\n",
    "    for class_name in CIFAR10_CLASSES:\n",
    "        print(f\"  {class_name}: {metrics[f'class_{class_name}_auprc']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved metrics\n",
    "metrics_dir = \"metrics\"\n",
    "all_loaded_metrics = {}\n",
    "\n",
    "for model_name in ['hard_baseline', 'mixed_baseline', 'artificial_soft']:\n",
    "    metrics_path = os.path.join(metrics_dir, f\"{model_name}_metrics.json\")\n",
    "    with open(metrics_path, 'r') as f:\n",
    "        all_loaded_metrics[model_name] = json.load(f)\n",
    "\n",
    "print(all_loaded_metrics.keys())\n",
    "for model_name, metrics in all_loaded_metrics.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs229_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
